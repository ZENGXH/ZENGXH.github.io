<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>




  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="XXXH">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="XXXH">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="XXXH">
<meta name="twitter:description">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>



  <title> XXXH </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">XXXH</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            Tags
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/31/Pattern-classification-and-machine-learning/" itemprop="url">
                Pattern classification and machine learning project I
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-31T21:32:03+01:00" content="2015-10-31">
            2015-10-31
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>In project I, we are given two dataset for regression and classification. In both of the dataset, there are (x_train, y_train) pair and x_test which is left for us to make prediction. Both of the x_train in two dataset are in high dimension(71 and 37 respectively). We need to perform data analysis, feature processing and model selection on the dataset. Also noticed that we are not provided any description on the data, ie, the data do not have any semantic meaning for us. </p>
<h2 id="This_notes_is_to_mark_down_the_detail_of_the_project_for_reference">This notes is to mark down the detail of the project for reference</h2><h1 id="regression">regression</h1><h2 id="data_description:">data description:</h2><p>x_train: 2800x71 dimension, ie, 2800 samples in 71 dimension<br>y_train: 2800x1<br>x_test: 1200x71, ie there are 1200 input for the predition</p>
<h1 id="classification">classification</h1><h2 id="data_description:-1">data description:</h2><p>x_train: 1500x37 dimension<br>y_train: 1500 binary {-1,1}<br>x_test: 800x37</p>
<p>comment: projectI 做完的感觉的就是，数据在手里的时候，很多theory的东西都fail了.比如说degree增大的时候lambda反而更小了，test error不是永远比train error小的。 logistic regression假设是input 是gaussian分布的但是实际上input可以很极端很skew。bias variance analysis倒还好，感觉用来解释error分布还是说得通的。但是lambda就很说不通…lambda的取值不太稳定感觉还是有点tricky.<br>这次的regression和classification都用了model split的方法，都是有一两个feature分布很典型，regression里用到了先做classification分成三个model再做regression的方法，error从500+一直降到了个位数。开心得晚上差点就通宵了。<br>还有dummy encoding用在两个dataset上面都是没什么效果。就是feature selection这种东西真的是很难有通用的方法啊。还有就是师姐太厉害了要在这里膜拜一下！</p>
<p>Below is what I planned to do at the beginning of the project.<br>Later I found that the most important things should be: </p>
<h3 id="First_apply_simple_model-">First apply simple model.</h3><h4 id="Then_really_look_at_the_data_plot!_such_as_histogram_and_scatter">Then really look at the data plot! such as histogram and scatter</h4><hr>
<ol>
<li>Exploratory data analysis: to learn about the characteristics of the data.<br>To have a general idea of the data, there are several things we can do:<br>1) try to plot the data to view the distribution of the data<br>2) try to use linear regression to fit the dataset(it is better to choose with the simplest approach as the start point)<br>3) base on what we find on step1 and step2, it is time to perform some feature processing on the dataset, <img src="" alt="Why feature processing is so important"><br>4) test on different models, it can be: a. linear regression with basis function expansion(polynormial is one the most simplest one, it can also be some special function such as sigmoid, sin, cos, exp and so on). b. as the complexity of the model increase, overfitting may arise, so we may consider perform regulization on the model, ie, try to penalize the weights, in this case, we can try ridge regression which use l2 regulization. (Notice that we can also use l1 regulization, however, according to what I read, people said that l2 is more stable then l1 while l1 can penalized the weights of the unuseful features to zero significantlt which may be helpful for us to figure the relation or information the features. However I have not look into the detail and proof yet, just mentioned here for future reference)<br>5) for the regulazation part, cross validation can be perform for parameters choice. another information cross validation can provide is the stability of our model, ie, if the performance/test error/train error have large variance on different cross fold, it may means the models are not that stable(my guess…), since that the performance is highly depend on the training data we choose. (overfitting with high variance? <em>not sure</em>) and<br>6) <img src="http://web.engr.oregonstate.edu/~tgd/classes/534/slides/part9.pdf" alt="bias-variance analysis"> ie, estimate the true test error by decomposite it into bias + variance part. and model comparision</li>
</ol>
<ol>
<li><p>Feature processing: cleaning your input and ouput variables, i.e. rescaling, transformation, removing outliers etc.</p>
</li>
<li><p>Applying methods and visualizing.<br> least square - regression without penelized</p>
<pre><code>worked <span class="keyword">or</span> <span class="keyword">not</span> worked
reason?
what <span class="keyword">is</span> beta, what beta shows us? <span class="keyword">the</span> weight?
</code></pre><p> ridge regression - how to choose lambda</p>
<pre><code>corss-validation
</code></pre></li>
<li>Determining whether a method overfits or underfits.</li>
<li>Cross-validation to estimate test errors.</li>
</ol>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/31/what-Bias-Variance-decomposition-brings-to-us/" itemprop="url">
                what Bias-Variance decomposition(BVD) brings to us
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-31T09:32:03+01:00" content="2015-10-31">
            2015-10-31
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>During project I finally gain more insight of what BVD means.<br>In the following I will briefly introduce what is BVD and the commend in parathesis is my point of view.<br>First of all, there are several question we need to pay attention to: </p>
<h2 id="what_is_the_measurement_of_the_model_complexity?">what is the measurement of the model complexity?</h2><ul>
<li>we may simply think that in linear regression with polynormial basis function expansion, as the degree increase, the model complecity increase. However, if the regulization is added, things change! Degree is not the only measument. when the regulization term is large, the weights are tend to zero(expect beta0), so the model is indeed simple even if the degree may be high.<ul>
<li>in a word, when measure the complexity of the model, it is better to keep one variable(ie, the degree or the regulazation) fixed. More specifically, for linear regression, it is simply depend on degree, for ridge regression, we usually fix the degree and vary lambda.</li>
</ul>
</li>
</ul>
<h2 id="what_the_definition_of_the_bias?">what the definition of the bias?</h2><ul>
<li>intuitively, if we mannully add our assumption(which may come from experience or the truth we know, it can be correct or wrong, benifit or harm to the model) to the model, the model will tend to depend less on the data we collect, then we will say the bias is high. The extreme case is that, we choose a large regulization term and all weights(expect beta0) go to zero, the models do not ‘listen’ to the data(the truth) and it is high bias.</li>
<li>specifically, the definition of bias is $bias^2 = (\mu(h<em>{hypothesis}(x^*)) - f</em>{truemodel}(x^*))^2$<ul>
<li>describe the average error of the hypothesis</li>
</ul>
</li>
</ul>
<h2 id="what_the_definition_of_variance?">what the definition of variance?</h2><ul>
<li>def: $variance = E[(h<em>{hypothesis}(x^*) - \mu(h</em>{hypothesis}(x^*)))^2]$<ul>
<li>it is the stability/variance of our model/hypothesis, if our model do not ‘listen’ to the data, then no matter what tranining set it use, it do not variant a lot and the value is low. On the other hand, if the model is overfitting(high complexity), then the variance is high.</li>
<li>in other word, it describe how much $h(x^*)$ differ from others</li>
</ul>
</li>
</ul>
<h2 id="under_what_assumption_do_we_perform_the_BVD?">under what assumption do we perform the BVD?</h2><ul>
<li>we assume that there exist a true model, even though we can nerver generate/get it.</li>
<li>also assume that there is some noise in the data we collect, ie $y^<em> = f(x^</em>) + e$(e: noise)<ul>
<li>notice that under this assumption, $E(y^<em>) = f(x^</em>) = \mu(y^*)$</li>
</ul>
</li>
</ul>
<h2 id="why_BVD_is_important_for_model_selecton?">why BVD is important for model selecton?</h2><ul>
<li>the measurement of whether a model is good is based on the test error, since that we do not know the true value of the data we need to predict, we can not compute the test error directly and we need to extimate it by testing on the sample data. <ul>
<li>the Estimation can be decomposited into three term: $bias$ + $variance^2$ + $noise^2$, <img src="http://web.engr.oregonstate.edu/~tgd/classes/534/slides/part9.pdf" alt="detail"></li>
<li>therefore the tradeoff between B and V is important</li>
</ul>
</li>
</ul>
<h2 id="how_the_tradeoff_happens?">how the tradeoff happens?</h2><ul>
<li>when the complexity of the model increase, in general, the trainning error will decrease since it tend to highly fitting into the trainning set(overfitting) and the test error will increase. <ul>
<li>Reason: when the input is away from the exact line(ie, there is some noise in the input), the output will become very different from the true value(ie, the noise will have huge impact on our prediction, also if there are noises in the trainning set, our model will be vary a lot and very different from the true model)<br>In this case, the bias is low(refer to the def of bias here), more specifically, the trainning error is ‘always’ low. </li>
</ul>
</li>
</ul>
<hr>
<h2 id="So_how_do_we_measure_B_and_D_in_practice_??_[not_finish]">So how do we measure B and D in practice ?? [not finish]</h2><p>to get $E(h(x^<em>))$ need more than one traning set. but only one indead, so need to simulate multiplr trainning sets by </em>bootstrap repicates* ie, randomly split the sample data into trainning and test for many times with different seeds(in MATLAB)</p>
<ul>
<li>algorithm:</li>
</ul>
<ol>
<li>given training sample S, set s seeds to split it</li>
<li>genarate multiple hypothesis h1,h2,h3,….hs</li>
<li>(option) determining corresponding weights w1,w2,…wL, here we view each h qually, ie. no weights for different hypothesis</li>
<li>classify new points by $y_{pred} = h<em>i(x</em>{new})$<br>$E(h(x^<em>)) = \mu(h(x^</em>)) = mean(\sum (h_i(x^<em>)) ) = \frac{1}{s}\sum (h_i(x^</em>))$,<br>$i \in {1,2,3,…s}$</li>
</ol>
<p>since that<br>$bias^2 = (\mu(h(x^<em>)) - f_{truemodel}(x^</em>))^2$<br>$bias = (\mu(h(x^<em>)) - f_{truemodel}(x^</em>))$</p>
<p>$$variance = E[(h(x^<em>) - \mu(h(x^</em>)))^2] =Var(h(x^<em>)) = Var(h(x_1^</em>)) + Var(h(x_2^<em>)) + …=  \frac{1}{s} </em>\Sum{E[(h_i(x^<em>) - \mu(h(x^</em>)))^2]}$$<br>when shown in the figure, it is the variance of different result(test error) of different seeds.</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> s = 1,2,3....</span><br><span class="line">	S = (Tr(s),<span class="keyword">Te</span>(s));</span><br><span class="line">	traning <span class="keyword">on</span> Tr(s)</span><br><span class="line">		compute errorTr(s)</span><br><span class="line">		<span class="comment">// for traning the parameters</span></span><br><span class="line">	<span class="keyword">test</span> <span class="keyword">on</span> <span class="keyword">Te</span>(s)</span><br><span class="line">		compute errorTr(s) </span><br><span class="line">		<span class="comment">// expect train(on all the sample we have) error E[(h(x^*) - y^*)^2]</span></span><br></pre></td></tr></table></figure>
<hr>
<h1 id="how_to_reduce_variance:">how to reduce variance:</h1><p>bagging and other resampling technique, one typical example is random forest<br>it means we preduce many times choosing different training set and get the average prediction as the result</p>
<hr>
<h1 id="insight">insight</h1><ol>
<li>bagging of a flexible(overfitting) models may reduce the variance and benift from low bias</li>
</ol>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/19/learning theory/" itemprop="url">
                learning theory - bias-variance tradeoff
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-19T21:32:03+02:00" content="2015-10-19">
            2015-10-19
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>看learning theory的时候，关注的是这个hypothesis怎么样， 而不是specific parameterization of hypotheses or whether it is linear classification<br>so we define <strong>hypothesis class H</strong></p>
<ul>
<li><strong>training error/empirical risk/empirical error</strong> of hypothesis h<ul>
<li>training set have size N, </li>
<li>assumption 1: (one of PAC assumption)<ul>
<li>training examples $(x^{(i)},y^{(i)})$ are drawn iid from some probability distribution D<br>$\hat{e}(h) = 1/N \sum_{i=1,2…N} (e_i)$<br>then we can have</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>generalization error</strong> </p>
<ul>
<li>DEF: under assum1, it is the probability that, if we now draw a new example (x, y) from the distribution D, h will misclassify it.</li>
<li>have two component: bias and variance</li>
</ul>
</li>
<li><p><strong>training error</strong></p>
<ul>
<li>the process of minimizing training error: empirical risk minimization (ERM)<ul>
<li>think of ERM as the most “basic” learning algorithm</li>
<li>logistic regression is approxi of ERM</li>
</ul>
</li>
</ul>
</li>
<li><p>expected train error</p>
<ul>
<li>by taking expectation over all possible training datasets of size N.</li>
<li>which is means we train for infinite datasets and take the average, but we cant, so we estimate by say, having m training datasets in size N then avarage the training error of each set</li>
</ul>
</li>
<li><p>in-sample test error</p>
<ul>
<li>for one given test-pair</li>
</ul>
</li>
<li><p>test error </p>
<ul>
<li>taking expectation over the test-data(all in-sample test error)</li>
</ul>
</li>
<li><p>expected test error</p>
<ul>
<li>average over all possible training data of size N, again we cant, so esitimate it by our limited test set</li>
</ul>
</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/19/classification/" itemprop="url">
                classification
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-19T21:32:03+02:00" content="2015-10-19">
            2015-10-19
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="logistic_regression">logistic regression</h2><ul>
<li>hypotheses $h_{\theta}(x) = \sigma(\theta^Tx)$ </li>
<li>$\sigma(x)$ is the logistic function/sigmoid function</li>
<li>we can assume:　<ul>
<li>$P(y = 1 | x; θ) =h_{\theta}(x)$</li>
<li>$P(y = 0 | x; θ) =1 - h_{\theta}(x)$</li>
<li>it can be written compactly</li>
</ul>
</li>
<li><code>Assuming that the m training examples were generated independently</code><ul>
<li>we can write $L(\theta) = p(\mathbf{y}|X;\theta)$ as joint probability of p(y1)p(y2)… then take log, get log likehood function of $\theta$<ul>
<li>maximize it (mini its negative) by ..</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="by_gradient_descent_or_least_square">by gradient descent or least square</h3><ul>
<li>one thing to mention: the update rule for stochastic gradient rule here $$\theta_j := \theta<em>j + \alpha y^{(i)} − h</em>{\theta}(x^{(i)})x<em>j^{(i)}$$ it is almost the same as we saw in linear regression, but, $h</em>{\theta}(x^{(i)})$ here is a non-linear function of $\theta^Tx^{(i)}$ </li>
</ul>
<h3 id="by_newtom_method">by newtom method</h3><ul>
<li>find zero of first derivative</li>
<li>$\theta := \theta - H^{-1}∇_{\theta}l{\theta}$</li>
<li>Newton’s method typically enjoys faster convergence than (batch) gradient descent, and requires many fewer iterations to get very close to the minimum. One iteration of Newton’s can, however, be more expensive than one iteration of gradient descent, since it requires finding and inverting an n-by-n Hessian; </li>
<li>When Newton’s method is applied to maximize the logistic regression<br>log likelihood function ℓ(θ), the resulting method is also called <strong>Fisher<br>scoring</strong></li>
</ul>
<hr>
<h1 id="perceptron_learning_algorithm">perceptron learning algorithm</h1><p>Consider modifying the logistic regression method to “force” it to output values that are either 0 or 1 or exactly. instead of sigmoid(x) we now use: threshold function g(x) = 1,if x&gt;0 | 0,if x&lt;0</p>
<ul>
<li>notice that it is a very different type of algorithm than logistic regression</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/19/read-caffe/" itemprop="url">
                read caffe
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-19T08:59:41+02:00" content="2015-10-19">
            2015-10-19
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="content">content</h2><p>the source code of caffe:<br>Blobs</p>
<p>Layers</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/18/useful-git-command/" itemprop="url">
                useful git command
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-18T09:52:27+02:00" content="2015-10-18">
            2015-10-18
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>markdown some useful git command in case…<br>NEVER ADD BIG FILE&gt;100M OTHERWISE YOU WILL DIE….</p>
<ul>
<li>if your want to push your local branch to remote repo<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// if you <span class="operator"><span class="keyword">update</span> origin(remote) branch somewhere <span class="keyword">else</span>, want <span class="keyword">to</span> <span class="keyword">fetch</span> the <span class="keyword">update</span></span><br><span class="line">git <span class="keyword">fetch</span></span><br><span class="line">// <span class="keyword">then</span> can perform <span class="keyword">merge</span></span><br><span class="line"></span><br><span class="line">// very useful <span class="keyword">to</span> <span class="keyword">check</span> the branch infomation <span class="keyword">for</span> remote</span><br><span class="line">// another one <span class="keyword">is</span></span><br><span class="line">git <span class="keyword">show</span>-<span class="keyword">ref</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">de2138ba78c41d27f7c3ce96da6693833c35774d refs/heads/ft2mat</span><br><span class="line">d56dd7b77591ef21b7a21ed7cb3e2ee91b542d22 refs/heads/xh</span><br><span class="line">d56dd7b77591ef21b7a21ed7cb3e2ee91b542d22 refs/remotes/origin/HEAD</span><br><span class="line">75a02a5638c7fabd77be176898ec25f24e5c7816 refs/remotes/origin/features</span><br><span class="line">51553de1b544a7dde3e0e17607c3ca1038401525 refs/remotes/origin/workLocal</span><br><span class="line">d56dd7b77591ef21b7a21ed7cb3e2ee91b542d22 refs/remotes/origin/xh</span><br><span class="line">"""</span></span><br><span class="line">// it means that, <span class="keyword">now</span> we have <span class="keyword">local</span> branch <span class="string">`ft2mat`</span> but it <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">in</span> the remote repo, <span class="keyword">in</span> this <span class="keyword">case</span>, we <span class="keyword">are</span> <span class="keyword">not</span> able <span class="keyword">to</span> <span class="string">`push origin/ft2mat`</span> since the repo ft2mat does <span class="keyword">not</span> exist</span><br><span class="line">// you will meet:</span><br><span class="line"># fatal: <span class="string">'origin/ft2mat'</span> does <span class="keyword">not</span> appear <span class="keyword">to</span> be a git repository</span><br><span class="line"># fatal: Could <span class="keyword">not</span> <span class="keyword">read</span> <span class="keyword">from</span> remote repository.</span><br><span class="line"></span><br><span class="line">// <span class="keyword">then</span> run:</span><br><span class="line">$ git push <span class="comment">--all -u</span></span><br><span class="line"></span><br><span class="line">// <span class="keyword">show</span> <span class="keyword">ref</span>:</span><br><span class="line">$ git <span class="keyword">show</span>-<span class="keyword">ref</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">de2138ba78c41d27f7c3ce96da6693833c35774d refs/heads/ft2mat</span><br><span class="line">d56dd7b77591ef21b7a21ed7cb3e2ee91b542d22 refs/heads/xh</span><br><span class="line">d56dd7b77591ef21b7a21ed7cb3e2ee91b542d22 refs/remotes/origin/HEAD</span><br><span class="line">75a02a5638c7fabd77be176898ec25f24e5c7816 refs/remotes/origin/features</span><br><span class="line">de2138ba78c41d27f7c3ce96da6693833c35774d refs/remotes/origin/ft2mat</span><br><span class="line">51553de1b544a7dde3e0e17607c3ca1038401525 refs/remotes/origin/workLocal</span><br><span class="line">d56dd7b77591ef21b7a21ed7cb3e2ee91b542d22 refs/remotes/origin/xh</span><br><span class="line">"""</span></span><br><span class="line">// <span class="keyword">now</span> we can </span><br><span class="line">$ git push origin ft2mat</span><br><span class="line"></span><br><span class="line">!done</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<p>after clone a repo from others, we may want to build up our work on it without push our work to others’ repo<br><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// first<span class="instruction"> check </span>the origin</span><br><span class="line">$ git remote -v</span><br><span class="line">origin  &lt;other repo&gt;.....</span><br><span class="line"></span><br><span class="line">// then change the origin into ours</span><br><span class="line">$ git remote set-url origin &lt;https://github.com/ZENGXH/caff&gt;</span><br><span class="line"></span><br><span class="line">//<span class="instruction"> check </span>again:</span><br><span class="line">$ git remote -v</span><br><span class="line"><span class="string">""</span><span class="string">"</span><br><span class="line">origin  https://github.com/ZENGXH/caff (fetch)</span><br><span class="line">origin  https://github.com/ZENGXH/caff (push)</span><br><span class="line">"</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">!done</span><br></pre></td></tr></table></figure></p>
<hr>
<p>2015/11/2<br>git真的是要好好学<br>比如说：<br>在不确定自己的branch的情况下不要随便add和commit。<br>不add和commit的话checkout new branch不会失去本地文件的。</p>
<p>以后要注意多和master merge要不分支越走越远，很容易就回不去了…T T</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/17/caffe document/" itemprop="url">
                caffe document
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-17T21:44:43+02:00" content="2015-10-17">
            2015-10-17
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <h1 id="official_caffe_tour">official caffe tour</h1><ul>
<li>nets, layers and blobs</li>
<li>forward/backwards</li>
<li>loss</li>
<li>solver</li>
<li>layer catalogue</li>
<li>interfaces</li>
<li>data
          <div class="post-more-link text-center">
            <a class="btn" href="/2015/10/17/caffe document/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/17/MLE/" itemprop="url">
                MLE & MAP & Bayesian Estimation
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-17T21:32:03+02:00" content="2015-10-17">
            2015-10-17
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>MLE: Maximum likelihood estimate<br>MAP: maximum a posteriori </p>
<h1 id="MLE">MLE</h1><h2 id="difference_between_likelihood_and_probability">difference between likelihood and probability</h2><table>
<thead>
<tr>
<th>likelihood</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>density</td>
</tr>
<tr>
<td>hold $x$</td>
<td>known $θ$</td>
</tr>
<tr>
<td>function of $\theta$</td>
<td>function of $x$</td>
</tr>
<tr>
<td>estimate $\theta$ from given data x – MLE</td>
</tr>
</tbody>
</table>
<p>-if we have a probability model with parameters θ. and note that<br>$$<strong>p(θ|y)</strong>=\frac{<strong>p(y | θ) p(θ)</strong>}{p(x)}$$,$\$</p>
<ul>
<li>pack all parameters into a single vector $\theta = {\alpha,\sigma^2}$ and write the function:<br>$$f(y|x;θ)$$. (andraw suggest we write $f(y|x;θ)$ in stead of $f(y|x,θ)$ since $\theta$ is nor random)(it is said that y and $\theta$ are interchangble)<ol>
<li>When we view $p(\mathbf{y}|X;θ)$ as a <strong>density/probability/distribution</strong> of y/ function is varying in $y$: <code>given constant values x and $\theta$ in mind and what is distribution of y?</code> – the uncertainty</li>
<li>When we view it as <strong>a function of $\theta$ holding $X$ constant</strong>, <code>given the model relating each y^i and x^i, what is the best guess/estimate of $\theta$</code>, then it become a <strong>likelihood</strong> function of $\theta$</li>
</ol>
<ul>
<li>$$L(\theta)=L(\theta;X,\mathbf{y})=p(\mathbf{y}|X;\theta)= \prod_{i=1,2,…N}p(y^{(i)}|x^{(i)};θ)$$ (pay attention to L and p)</li>
<li>Then we turn to <code>pricipal of maximum likelihood</code> which says: <code>choose $\theta$ so as to make the data as high probability as possible!</code> then we now should choose $\theta$ to maximize $L(\theta)$<ul>
<li>lets look into $L(\theta)$: if we assume error is iid independent and follow Guanssian distribution with $\sigma^2$, then we can know the distribution of y(same as e) in order to exspan $p(y^{(i)}|x^{(i)};θ)$ – <code>keep in mind!!</code></li>
<li>we also take log for easier calculation which now called <code>log likelihood $l(\theta)$</code>, and trun find max to find mini of $$1/2 \sum_{i=1,2,..N}(e^(i))^2$$<ul>
<li><em>SURPISE!</em> the form is the same as $J(\theta)$ which we generate in MSE cost function! </li>
<li>which can be solve by gradient descent or least square</li>
</ul>
</li>
<li>additionally, for two condidate $\theta_1$ and $\theta_2$, the likelihood ratio is used to measure the relative likelihood</li>
</ul>
</li>
<li>commend: this view of the parameters as being <em>constant-valued but unknow</em> in taken in frequentist statistic<blockquote>
<p>(from andrew’s notes, keep for further understand)To summarize: Under the previous probabilistic assumptions on the data, least-squares regression corresponds to finding the maximum likelihood estimate of θ. This is thus one set of assumptions under which least-squares regression can be justified as a very natural method that’s just doing maximum likelihood estimation. (Note however that the probabilistic assumptions are by no means necessary for least-squares to be a perfectly good and rational procedure, and there may—and indeed there are—other natural assumptions that can also be used to justify it.) Note also that, in our previous discussion, our final choice of θ did not depend on what was $\sigma^2$ , and indeed we’d have arrived at the same result even if $\sigma^2$ 2 were unknown. We will use this fact again later, when we talk about the exponential family and generalized linear models</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>MLE</th>
<th>MAP</th>
</tr>
</thead>
<tbody>
<tr>
<td>find $\theta$ maximizes $p(x</td>
<td>θ)$</td>
<td>view $p(θ</td>
<td>x) 8 p(θ) p(x</td>
<td>θ)$ then find $\theta$ that maximizes $p(θ</td>
<td>x)$ == maximizing $p(θ) p(x</td>
<td>θ)$</td>
</tr>
<tr>
<td>comes from linear regression modle: $f(y</td>
<td>x,\alpha,\sigma)$</td>
<td>add regulazation(called <code>prior belief</code> in the probabilistic point of view), ie, ridge regression, we can view ridge regression as astimator of MAP</td>
</tr>
<tr>
<td>dis: overfitting, explain below</td>
</tr>
</tbody>
</table>
<ul>
<li><p><img src="https://www.quora.com/What-is-the-difference-between-Maximum-Likelihood-ML-and-Maximum-a-Posteri-MAP-estimation" alt="get from here"> MLE have problem of overfit the data, variance of the parameter estimates is high, or put another way, that the outcome of the parameter estimate is sensitive to random variations in data (which becomes pathological with small amounts of data). To deal with this, it usually helps to add regularisation to MLE (i.e., reduce variance by introducing bias into the estimate). </p>
</li>
<li><p>In maximum a posteriori (MAP), this regularisation is achieved by assuming that the parameters themselves are also (in addition to the data) drawn from a random process. The prior beliefs about the parameters determine what this random process looks like.</p>
</li>
</ul>
<hr>
<h1 id="MAP">MAP</h1><ul>
<li><p>contrasts with MLE, the <strong>maximum-a-posteriori or MAP estimate</strong>, </p>
<ul>
<li>which is the $\theta$ that maximizes $p(θ | x)$.  </li>
<li>Since x is fixed, this is <strong>equivalent</strong> to maximizing $p(θ) p(x | θ)$, the product of the <strong>prior probability</strong> of θ with the likelihood of θ.</li>
<li>the <strong>prior probability/belief</strong> is indeed the regulazation term in ridge regression, so is the prior belief is strong, then high bias and low var(data affect a little)</li>
</ul>
</li>
<li><p>for an infinite amount of data, MAP gives the same result as MLE (as long as the prior is non-zero everywhere in parameter space); </p>
</li>
<li>for an infinitely weak prior belief (i.e., uniform prior), MAP also gives the same result as MLE.</li>
<li>MLE can be silly, for example if we throw a coin twice, both head, then MLE asid you will always have head in the future. Bayesian have clever explain consider the assumtion that head come with 0.5 probability </li>
<li>MAP is the foundation for Naive Bayes classifiers</li>
<li>MAP is applied in spam filter while MLE can not</li>
</ul>
<hr>
<p>in Naive Bayes classifiers, we assum: <code>features are conditionally independent</code><br>and use empirical probabilities:<br><em>prediction = argmaxC P(C = c|X = x) ∝ argmaxC P(X = x|C = c)P(C = c)</em><br>example:<br>$P(spam|words) ∝ \prod_{i=1,2..N}P(words<em>i|spam)P(spam)$<br>$P(~spam|words) ∝ \prod</em>{i=1,2..N}P(words_i|~spam)P(~spam)$<br>Whichever one’s bigger wins.</p>
<ul>
<li><p>dis: sampling is important, may blow up thind is we train on data mostly spam and test on mostly non-spam(our P(spam) is WRONG) – but we can perfrom cv to adviod this</p>
</li>
<li><p>modify NB:  joint conditional distribution</p>
</li>
<li>decision surface of Naive Bayes: P(c|word) = P(word|c)I(word) + P(¬word|c)I(¬word)</li>
</ul>
<hr>
<h1 id="Bayesian_estimation">Bayesian estimation</h1><ul>
<li>using Bayes’ rule, come up with a distribution of possible parameters:<br>$P(\mathbf{\theta} | \mathbf{D}) =\frac{ P(\mathbf{D}|\mathbf{\theta}) p(\mathbf{\theta})}{P(\mathbf{D})}$</li>
<li>p(\mathbf{\theta})} is known as prior(it means we make some assumption of the parameters, or, we somehow know some fact such as the coin have 0.5 changes of getting head. but the assumption may be wrong obviously?)</li>
<li>KEEP IN MIND: ${P(\mathbf{D})}$ is constant</li>
<li>we need to intergral two side w.r.t $\theta$ which have high computational cost</li>
</ul>
<hr>
<h2 id="probability_or_statistic">probability or statistic</h2><p>In probability, we’re given a model, and asked what kind of data we’re likely to see.<br>In statistics, we’re given data, and asked what kind of model is likely to have generated it.</p>
<h2 id="least_square">least square</h2><ul>
<li>The method of least squares is a standard approach in regression analysis to the <strong>approximate solution of overdetermined systems</strong>, i.e., sets of equations in which there are more equations than unknowns.</li>
<li>if all residual are linear, then it is linear least square: </li>
<li>The linear least-squares problem occurs in statistical regression analysis;</li>
<li>it has a closed-form solution.</li>
</ul>
<h2 id="fisher_information">fisher information</h2><ul>
<li>Fisher information is a way of <strong>measuring the amount of information that an observable random variable X carries about an unknown parameter θ of a distribution that models X</strong>. Formally, it is the variance of the <strong>score</strong>, or the expected value of the observed information. </li>
</ul>
<h2 id="The_likelihood">The likelihood</h2><ul>
<li>defined as the <strong>joint density or probability of the outcomes</strong>, with the roles of the values of the outcomes y and the values of the parameters θ interchanged. $p(y|x,\theta)$, $p(\theta|y)$</li>
<li><code>score function</code> = <code>derivative of the log-likelihood</code>:<ul>
<li>find the maximum likelihood estimator, set the score function equal to zero</li>
</ul>
</li>
</ul>
<h3 id="maximum_likelihood_estimator">maximum likelihood estimator</h3><ul>
<li>The maximum likelihood estimator of θ for the model given by the joint densities or probabilities $f(y;\theta)$, with $\theta \in Θ$, is defined <strong>as the value of θ at which the corresponding likelihood L(θ;y) attains its maximum</strong></li>
<li>$\hat \theta<em>{ML} = argmax</em>{\theta} L(\theta;y)$</li>
</ul>
<h4 id="efficiency_of_the_maximum_likelihood_estimators">efficiency of the maximum likelihood estimators</h4><ul>
<li>relate to asymptotic settings</li>
<li>A ubiquitous caveat associated with all the results is that the model has to be valid; </li>
<li>it has to contain the distribution according to which the outcomes are generated.</li>
<li><strong>Consistency</strong> <ul>
<li>a property of an estimator </li>
<li>that it would recover the value of the target if it were based on many observations. </li>
<li>ie.  we refer to the sequence of (univariate) estimators $\hat θ_n$ based on thenth set of observations yn as a single estimator. Consistency of such an estimator ˆ θ of a target θ is defined as convergence of ˆ θnto the target θ as n→+∞. </li>
<li>for MLE:<ul>
<li>An important result about maximum likelihood estimators is that under some regularity conditions they are consistent. The regularity conditions include smoothness of the likelihood, its distinctness for each vector of model parameters and finite dimensionality of the parameter space, independent of the sample size. </li>
</ul>
</li>
<li>Asymptotic Efficiency and Normality<ul>
<li>The qualifier asymptoticrefers to properties in the limit as the sample size increases above all bounds. </li>
<li>For a set of many conditionally independent outcomes (large sample size n), given covariates and a finite-dimensional set of parametersθ, the maximum likelihood estimator is approximately unbiased, and its distribution is well approximated by the normal distribution with sampling variance matrix equal to the inverse of the expected information matrix. This result is referred to as asymptotic normality. Further, the maximum likelihood estimator isasymptotically efficientand, asymptotically, the sampling variance of the estimator is equal to the corresponding diagonal element of the inverse of the expected information matrix. That is, for largen, there are no estimators substantially more efficient than the maximum likelihood estimator.<br><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml3-10.png" target="_blank" rel="external">http://www.52nlp.cn/wp-content/uploads/2015/01/prml3-10.png</a></li>
<li>TheCram´er–Rao inequalityis a powerful result that relates to all unbiased estimators. It gives a lower bound for the variance of an unbiased estimator.</li>
<li>Asymptotic normality and efficiency of the maximum likelihood estimator confer the central role on the normal distribution in statistics.</li>
<li><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml3-10.png" target="_blank" rel="external">http://www.52nlp.cn/wp-content/uploads/2015/01/prml3-10.png</a></li>
</ul>
</li>
</ul>
</li>
<li>Instead of $L(\theta; y,X)$ it is more convenient to work with its logarithm, called the log-likelihood – product convert into summation</li>
</ul>
<h2 id="information_matrix/_fisher_information">information matrix/ fisher information</h2><ul>
<li>Fisher information is used to determine the sample size with which we design an experiment; second, in the Bayesian paradigm, Fisher information is used to define a default parameter prior; finally, in the minimum description length paradigm, Fisher information is used to measure model complexity</li>
</ul>
<hr>
<p>machine learning经常用到probability和statistic的解释和一些概念，感觉看起来一模一样的东西又可以有很多不同解释..如果google的话强烈推荐quora啊太良心了，如果是stackexchange的话经常会发生努力看完top回答然后下面来一个it’s totally wrong简直是人生观都要颠覆了。 </p>
<hr>
<ul>
<li>任意一个函数 可以放到 ${1,x,x^2,x^3….}$所张成的函数空间，如果是有限个基的话就称为欧式空间，无穷的话 就是 Hilbert空间</li>
<li><a href="http://www.52nlp.cn/wp-content/uploads/2015/01/prml3-25.png" target="_blank" rel="external">http://www.52nlp.cn/wp-content/uploads/2015/01/prml3-25.png</a></li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/17/prml/" itemprop="url">
                prml
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-17T21:32:03+02:00" content="2015-10-17">
            2015-10-17
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>life is so hard.</p>
<ol>
<li>logistic regression is not applied for regression</li>
<li>least mean square is a rule(the update rule for gradient descent)</li>
<li>least square is actually means: using MSE distance function, it can be solve by Gradient descent or normal equation</li>
<li>maximum likelihood exrimate of $\theta$ is solve by minimize the negative of log-likehood function of $\theta$</li>
<li>likehood may be view intuitively as the likelihood of what y will be given x but when talking MLE it is actually the likelihood of what parameters will be gien x (and y, or distrubution of y)</li>
<li>sigmoid function is a magic(fourier is also a magic): $\sigma(x)’ =\sigma(x)(1-\sigma(x))$</li>
<li>if you hold strong prior belief, you will have high bias and low variance, the world will not affect you…</li>
<li>MLE can be really silly.. due to overfitting. <img src="https://www.quora.com/Intuitively-speaking-What-is-the-difference-between-Bayesian-Estimation-and-Maximum-Likelihood-Estimation" alt="see here"></li>
<li>under a set of assumptions(Gaussian, independent) least squares regression could be derived as the maximum likelihood estimator </li>
<li><h2 id="preprocessed">preprocessed</h2>for input may be <em>preprocessed</em><ul>
<li>like fixed size of the image</li>
<li>also called feature extraction</li>
<li>speed up computation</li>
<li>easier to solve problem</li>
</ul>
</li>
</ol>
<h1 id="reference_book">reference book</h1><ol>
<li>PRML:</li>
<li>machine learning, a probalistic approach: murphy</li>
<li>the elements of statistical learning:HTF</li>
<li><img src="http://cs229.stanford.edu/notes/cs229-notes1.pdf" alt="Andrew&#39;s notes"></li>
</ol>
<h1 id="OVERVIEW">OVERVIEW</h1><ul>
<li>for the <code>data input type</code> we can define the learning process as supervised or unsupervised</li>
<li>for the <code>type of prediction</code>, we can define the case as classification(discrete output) or regression(continuous output)</li>
<li>for the <code>fitting model</code> we choose, we can have linear model(basis function expansion), or rigistic regression(not regression but for classification actually)</li>
<li><p>analyze the model and find parameter – cost function:</p>
<ul>
<li><p>derectly define <code>cost function</code> based on distance such as MAE, MSE…</p>
<ul>
<li>whether the cost function is <code>robust</code>, ie, how they react to the <code>outlier</code> will also affect our model – statistic property</li>
<li>whether the cost function is <code>convex</code> relate whether we can reach the minimum if we count on the gradient to solve the minimum – computational property</li>
</ul>
<ul>
<li>just based on the cost function above is not enough, we can also take another approach..</li>
</ul>
</li>
<li><p>define from <code>probabilistic view</code> of machine learning, we try to seek support for our modeling precess, analyze the model and tuning the model,  </p>
<ul>
<li><p><code>probability theory</code> can be applied during the prediction, model selection, measurement to be performed</p>
</li>
<li><p>for regression, we focus on the distribution of the output data or error </p>
<ul>
<li>by assume the error following the Gaussian distribution, the value we try to estimate/predict also follow the Gaussian distribution which take the y_prediction as mean.</li>
</ul>
</li>
<li>while for classification, we take the Bayisan theorm – and then generate the likehood function</li>
<li>we can use the joint probability assuming the data is indenpendent</li>
</ul>
</li>
<li><p>WHY cost function so important:</p>
<ul>
<li>since that the parameter is choosen by the score return by cost function </li>
</ul>
</li>
</ul>
</li>
<li>consider whether the <code>parameter is fixed or changed</code> with the size of data, the model can be <code>parametric</code>(linear regression(oridinary), logistic…) or <code>nonparametric</code>(<img src="http://cs229.stanford.edu/notes/cs229-notes1.pdf" alt="Locally weighted linear regression">(only meet at andrew’s, not the linear-regress we saw),KNN)       </li>
<li>for <code>optimation</code> numerically ie. <code>solve the minimum</code> of function in which we regard the parameter instead of the data as variable, we have: gradient descent, grid search, least square(closed form soluction, Gram matrix), newton method. for the solver, we need to consider the <code>compuration complexity</code> and <code>convexity</code><ul>
<li>implementation issue: – see reference: non-linear programming</li>
<li>gradient descent: feature normalization, direction(batch gradient descent, stochastic), stepsize(fixed, line-search:backtracking), convexity and converge, stopping criteria(optimal condition, positive definite)</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>supervised learning<ul>
<li>classification- discrete output</li>
<li>regression- continues output</li>
<li><code>generalization</code> – analyzing model<ul>
<li>:= make predictions on novel input</li>
<li>generalization error = expected value of the misclassification rate when averaged over future data, can never know but can be approximate by:  misclassification rate on a large independenttest set</li>
</ul>
</li>
</ul>
</li>
<li>unsupervised learning<ul>
<li>goal: knowledge discovery or discover interesting sreuctre in data in murphy</li>
</ul>
<ul>
<li>visualization</li>
<li>clustering</li>
<li>density estimation<br>…<br>-reinforcement learning</li>
<li>find suirable actions to take in a given stituation in order to maximized the reward</li>
</ul>
</li>
</ul>
<h2 id="linear_model">linear model</h2><ul>
<li>function which is linear in unknow parameters are called <code>linear model</code><ul>
<li>for the <code>polynomial model</code>: </li>
<li>$y(x, \mathbf{\beta}) = \beta^0 + \beta_1 x + \beta_2 x^2 + …. + \beta_M x^M$ </li>
<li>notices that the basis function(polynomial function) is <em>nonliear</em> in input x but linear in unknow parameter $\beta$</li>
<li>implement issue/ model selection:[detail in <em>ovrifitting</em> part]<ul>
<li>choose the value of coefficents/parameters/weights is determined by fitting data - minimizing error function - <em>cost function</em></li>
<li>choose the order/degree M - model selection/comparision<ul>
<li>overfitting </li>
<li>regulazation</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>in linear regression: take $\phi$ as veriable – linear; but the $\phi$ function itself can be nonliear</li>
</ul>
<hr>
<h1 id="probability_theory">probability theory</h1><ul>
<li>提出probability 是想更加科学一些</li>
<li><em>expectation</em> <ul>
<li>def: weighted averages of funtions</li>
<li>if we are given a finite number N of points drawn from the probability distribution or probability density<ul>
<li>$E[f] ~- 1/N sum_{n=1,2…N}f(x_n)$       </li>
</ul>
</li>
<li>consider expectation of functions of several variables eg f(x,y)<ul>
<li>expectation have subscript with repesct to different variable:</li>
<li>$E_x[f(x,y)]$ and $E_y[f(x,y)]$</li>
</ul>
</li>
</ul>
</li>
<li>variance: $Var[x] = E[x^2] - E[x]^2$<ul>
<li>consider functions of several variables eg f(x,y)<ul>
<li><em>covariance</em>: $cov[x,y] = E_{x,y}[{x - E[x]}{y^T - E[y^T]}]$  </li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="background:_interpretation_of_probabilities">background: interpretation of probabilities</h2><h3 id="frequentist_interpretation">frequentist interpretation</h3><ul>
<li>popular: <em>classical or frequentist way</em><ul>
<li>the probability P of an uncertain event A, P(A) is defined by the frequency of that event based on previous observations</li>
<li>In the frequentist this view of the world, θ is not random—it just happens to be unknown—and it’s our job to come up with statistical procedures (such as maximum likelihood) to try to estimate this parameter.(NG)</li>
</ul>
</li>
</ul>
<h3 id="Bayesian_interpretation">Bayesian interpretation</h3><ul>
<li>another point of view: <em>Bayesian view</em> - <code>probability provide a quanrification of uncertainty</code><ul>
<li>ads: can be used to model our uncertainty of the event without long term frequency</li>
<li>for future event, we do not have historical database thus can not count the frequency.</li>
<li>can measure the belief in a statement $a$ based on some ‘knowledge’, denote as P(a|K), different K can generate different P(a|K) and even same K can have different P(a|K) – the belief is subjective </li>
</ul>
</li>
</ul>
<h4 id="Bayes_rule">Bayes rule</h4><ul>
<li>consider <code>conditional probabilities</code> </li>
<li>$P(A|B) = \frac {P(B|A) P(A)}{P(B)}$<ul>
<li>interpretation: updating our belief about a hypothesis A in the light of new evidence B<ul>
<li>in <code>likelihood</code>, it is, output brief of y/A given B/input values+paramters </li>
</ul>
</li>
<li>P(A|B): posterior belief</li>
<li>P(A): prior belief</li>
<li>P(B|A): likelihood, ie the B(our model) will occur if A(the output value of the sample data) is true.</li>
<li>P(B) is computed by: $\sum_{i=1,2,…}P(B|A_i)P(A_i)$ by <em>marginalisation</em>.</li>
<li>if you still remember the ‘cancer-test examples’ in statistic course, then p(A) is p(cancer), p(B|A) is p(positive|cancer), p(A|B) is p(cancer|positive) which is the value patient care for(but in fact we dont know) </li>
</ul>
</li>
</ul>
<h2 id="different_view_of_likelihood_function">different view of likelihood function</h2><ul>
<li>likelihood function: $P(\mathbf{y}|\mathbf{\beta})$</li>
</ul>
<h3 id="from_frequentist_way_of_interpretation:">from frequentist way of interpretation:</h3><ul>
<li>parameter $\beta$ is a fixed parameter, the value is determined by ‘estimator’</li>
<li>A widely used <code>frequentist estimator</code> is <em>maximum likelihood</em>, in which we set the value that maximizes the likelihood function </li>
<li>ie. choosing $\beta$ s.t. probability of the observed data is maximized(seems to be the MAP estimate introduced in Murphy’s book, see below)</li>
<li>in practice, use negative log of likelihood function = log-likelihood:= error function(monotonically decreasing)</li>
<li><strong>One approach to determining frequentist error bars is the bootstrap</strong>, <ul>
<li>s1: 就是在已有的dataset(size N)里面random弄出L个dataset(size N) by drawing data from 已有的dataset(抽取方式是，可以重复抽，可以有的没有被抽中)</li>
<li>s2: looking at the variability of predictions between the different bootstrap data sets. then evaluate the accuracy of the estimates of the parameter</li>
</ul>
</li>
<li>drawback: may lead to extreme conclusion if the dataset is bad, eg, a fair-looking coin is tossed three times and lands heads each time. in this case, we will generate parameter $\beta$ to make P(lands head) = 1   </li>
</ul>
<h3 id="from_Bayesian_viewpoint:">from Bayesian viewpoint:</h3><p>in machine learning, Bayes theorem is used to convert a priot ptobability $P(A) = P(\mathbf{\beta})$ into a posterior probability $P(A|B) = P(\mathbf{\beta}|y)$ by incorpoating the evidence provided by the observed data </p>
<ul>
<li>for $\mathbf{\beta}$ in the <code>polynormial curve fitting model</code>, we can take an approach with <code>Bayes theorem</code>:</li>
<li>$P(\mathbf{\beta} | \mathbf{y}) =\frac{ P(\mathbf{y}|\mathbf{\beta}) p(\mathbf{\beta})}{P(\mathbf{y})} $<ul>
<li>given data {y_1,y_2,…}, we want to know the $\beta$, cant get directly. $P(\mathbf{\beta} | \mathbf{y})$:= <strong>posterior probability</strong></li>
<li>$P(\beta)$:= prior probability; our assumption of $\beta$</li>
<li>${P(\mathbf{y})}$:= normalization constant since the given data is fixed</li>
<li>$P(\mathbf{y}|\mathbf{\beta})$:= <strong>likelihood function</strong>;<ul>
<li>can be view as function of parameter $\beta$ </li>
<li>not a probability distrubution, so intergral w.r.t $\beta$ not nessary = 1</li>
</ul>
</li>
<li>state Bayes theorem as : <strong>posterior 8 likelihood × prior</strong>, consider all of these as function of parameters $\beta$<ul>
<li>about the choice of the parameters <code>murphy have detail description in chapter3</code> but should notice that murphy have different notation for the relation above…<ul>
<li>intergrate both side base on $\beta$: $p(y)= \integral p(y|\beta)p(\beta)d\beta$</li>
<li>issue: particularly the need to marginalize (sum or integrate) over the whole of parameter space</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Murphy’s_explanation_–_MAP(maximum_a_posterior)_estimate">Murphy’s explanation – MAP(maximum a posterior) estimate</h3><ul>
<li>look at probability is conditional on the test input x, as well as the training set D, and the form of model M that we use to make predictions</li>
<li>Given a probabilistic output, we can always compute our “best guess” as to the “true label” using: <ul>
<li>$\hat y = \hat f(\mathbf{x} = argmax_{c=1..C} p(y = c|\mathbf{x},D,M)$</li>
<li>This corresponds to the most probable class label, and is called the mode of the distribution p(y|x,D); it is also known as a <strong>MAP estimate</strong>(maximum a posterior). </li>
</ul>
</li>
<li>note that p(y|x,D) is the <code>probrbility distribution of y given x and D</code> (and in case of in the situation of model selection, M should also be clearified, write as p(y|x,D,M))</li>
</ul>
<hr>
<h2 id="Murphy’s_definition_of_probabilistic_model_in_machine_learning">Murphy’s definition of probabilistic model in machine learning</h2><p>when defining a models, if the number of parameter is:</p>
<ol>
<li><p><code>grow with the amount of training data =&gt; non-parametric model</code></p>
<ul>
<li>flexible but computation cost high</li>
<li>eg: KNN<ul>
<li>KNN probabilistic model: </li>
<li>$p(y=c|\mathbf{x},D,K) = 1/K \Sigma_{i in N_K(x,D)} II(y_i=c)$</li>
<li>NK(x,D)are the (indices of the)Knearest points toxin DandI(e)is the indicator function </li>
<li>issure: fail to work in high D due to <code>curse of dimensionality</code>[SEE BELOW]</li>
</ul>
</li>
</ul>
</li>
<li><p><code>fixed =&gt; paramteric</code> </p>
<ul>
<li>ads: fast to apply</li>
<li>dis: strong assumption about the nature of data distribute</li>
<li>regression<ul>
<li>linear regression:<ul>
<li>model: $y(x)=w^Tx + \episilon$</li>
</ul>
<ul>
<li>assume $e~N(\mu,\sigma^2)$ connect linear regre and Gaussian:<ul>
<li>model: $p(y|\mathbf{x},\mathbf{w},\sigma^2)=N(t|\mathbf{w}^T \phi(\mathbf{x}),\sigma^2(x))$</li>
<li>$\phi(\mathbf{x})$ basis function expansion, usually it is a vector of different degree of x – in polynomial regression</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>classification:<ul>
<li>logistic regression, ber-distribute, use sigmoid function as basis function. <ul>
<li>define decision rule + basi function expansion -&gt;create decision boundary </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="the_curse_of_dimensionality_(murphy_ch1)">the curse of dimensionality (murphy ch1)</h3><ul>
<li>KNN fail to work in high D</li>
</ul>
<ul>
<li>explanation:<ul>
<li>lets say, apply KNN to uniformly distributed points in 3 dimensional unit cube grow a hyper-cube around x untill it contain 0.5 of the data points, $V(hyper-cube)=e_3(0.5)^3=0.5$, $e_3(0.5) = 0.5^{1/3}$, </li>
<li>for D dimension, given disired faction f, length of cube around x is $e_D(f)=f^{1/D}$</li>
</ul>
<ul>
<li>$e_{10}(0.1)=0.8$: D=10, want 0.1 of the datas, length=0.8, ie, need to extend the cube 80% along each dimension around x, which means that, the ‘nearest nerighbor’ is indeed not that near since that on any dimendion, their distance can be 0.8 and the largest distance is only 1; </li>
</ul>
</li>
</ul>
<ul>
<li>combat by making assumption about the nature of data distribution – know as <em>inductive bias</em> embodied in parametric model</li>
</ul>
<hr>
<h2 id="overfitting_and_model_selection">overfitting and model selection</h2><ul>
<li>overfit is an issue raised in both unsurpervised learning and surpervised learning during the selection of model</li>
<li>PRML said <code>by adopting Bayesian approach, the overfitting problem can be avoided and the num of parameters can even greatlt exceed the num of data point</code></li>
<li><code>overfitting problam can be regards as a general property of maximum likelihood</code>, the matrix tend to be always in ill-condition since that the data points can not be completely in linear relation in reality.</li>
</ul>
<h3 id="overfitting_in_linear_regression">overfitting in linear regression</h3><ul>
<li>in linear regression, if choose polynormial as basis function, then the degress of the polynormial function distinct the model, the higher the degree is, it is more likely to overfit, which will lead to high error on the test set, since we use this error to estimate the generalization error, it means that the generalization increase<ul>
<li>as the degree raise, the complexity of the model increase, the matrx will tend to be <code>ill condition</code>, and the solution of parameters will become more sensitive to the training data, which is really bad! </li>
<li>also notice the fact that, as degree increase, the value of parameter also increase dramatically, in the example of PRML, D=1,w0=0.19 while for D=9,w9=125201 !</li>
<li><code>commend</code> someone said when we are not sure about the model, we should always choose the simplier model to fit the data</li>
<li><code>commend</code> we can still have some way to use complex model and reduce the generalization error</li>
</ul>
</li>
</ul>
<h4 id="solution1:_shrinkage_method_/_regularization_to_fight_with_overfitting">solution1: <code>shrinkage method</code> / <code>regularization</code> to fight with overfitting</h4><ul>
<li><p>usually we will try to penilize the parameters of the model, which perform <code>lift of the eigrnvalue</code>, in this way, we are actually trying to simplify the models, (thinking that when the panelized parameter $\lambda$ bacomre very large, the parameters, except $\w_0$ which is nor panelized, are all going to zero, so we are using a very simple model in fact). </p>
<ul>
<li>involves adding a penalty term to the error function (1.2) in order to discourage the coefficients from reaching large values</li>
</ul>
<ul>
<li>add $\lambda / 2 | \mathbf{\beta} |^2$ $\lambda$ is the importance of the regulazation term; <ul>
<li>if use quadric regularizer - call <em>ridge regression</em></li>
<li>other regulazation such as <em>weight decay</em> in neural network </li>
</ul>
</li>
<li>$\lambda$ can suppressed over-fitting, reduce value of weight, but if $\lambda$ too large, weight goes to 0, will lead to poor fit</li>
</ul>
<ul>
<li>we should also notice that the panelized parameter $\lambda$ could very a lot with the change the degree, from my experiment, it can very in the 1 ~ 10^7 range as the degree goes from about 2 to 10. So when perform cross validation on the selection of the model, more specifically, on the decision of penalized perameter and the degree, we need to pay attention about this</li>
<li>after adding regularization, the complexity of the model should not be measured simply by the number of parameters or the degrees</li>
</ul>
</li>
</ul>
<h3 id="overfitting_in_KNN,">overfitting in KNN,</h3><ul>
<li>it also happens that when K is small, it tends to overfit.</li>
</ul>
<hr>
<h2 id="speed-accuracy-complexity_tradeoff">speed-accuracy-complexity tradeoff</h2><p>in the following notes, I will focus on the comparison of the tradeoff</p>
<ul>
<li>indicator function: $II(e)=1,if e is true; =2 if e is false$</li>
</ul>
<hr>
<h2 id="distribution">distribution</h2><h3 id="multinomial_distribution">multinomial distribution</h3><ul>
<li>MODEL: tossing K side die </li>
<li>$\mathbf{x}=(x_1,…,x_D)$, $x_j$ the number of times side x of the die occurs</li>
<li>notice that $N = sum(x_1,x_2,…x_D)$</li>
<li>ANOTHER MODEL: from D color, choose fill in N place(not consider the order, ie,red in place1+blue in place2=red in 2+blue in 1), $x_j$ the number of color j shows in the image. probability that j is chosen = $\theta_j$</li>
<li>$Mu(\mathbf{x}|N,\mathbf(\theta)) := \binom{N}{x_1} \binom{N-x_1}{x_2}..\binom{N-x_1-x_2-…}{x_D} * \theta_1^{x_1} \theta_2^{x_2}…\theta_D^{x_D}$</li>
<li>$= \binom{N}{x_1 x_2 ..x_D} \prod{\substack{j=1,2,…D}} \theta_j^{x_j}$</li>
</ul>
<h3 id="multi-noulli">multi-noulli</h3><ul>
<li>simply let N = 1;</li>
<li>$Mu(\mathbf{x}|1,\mathbf(\theta)) := \prod{\substack{j=1,2,…D}} \theta_j^{II(x_j=1)}$ II for 0,1 case</li>
</ul>
<hr>
<h2 id="information_theory">information theory</h2><ul>
<li>deta compression/ source coding, concerned with: compact data representation and errorless transmission and storing </li>
</ul>
<h3 id="entropy">entropy</h3><ul>
<li>for a random variable Y(with K state) with ditribution p, the entropy of Y is denoted by </li>
<li>$\mathbb{H}(Y)$ or $\mathbb{H}(p)$ </li>
<li>$:= - \sum_{\substack{k=1,2,….K}} p(Y=k) log_2 p(Y=k)$</li>
<li>is a measure of its uncertainty<ul>
<li>say if $X \in {1,…,5}$ then if p = [1/5,1/5,1/5,1/5,1/5] the abs(entropy) reach maximum and in this case, entropy(&lt;0) is minimum, therefore uncertainty is largest when p = [1,0,0,0,0] the netropy reach minimum - no uncertainty</li>
</ul>
</li>
</ul>
<hr>
<h2 id="feature_selection">feature selection</h2><p>motivation: having too many features and some of them are actuallt invariant, feature selection is belonging to model selection as well, introduced in notes 5 of Andrew NG</p>
<h2 id="regulazation">regulazation</h2><pre><code>- solve ill posed problem/ overfitting
<span class="number">1.</span> when talking <span class="keyword">about</span> MLE which <span class="keyword">is</span> alway facing overfitting problem, <span class="keyword">the</span> MAP <span class="keyword">and</span> baysian estimation <span class="keyword">is</span> improving <span class="keyword">it</span> <span class="keyword">with</span> prior belif which act <span class="keyword">as</span> regulazation 
</code></pre><hr>
<p>regular expression help to replace the wrong mathjex:<br>eg find: (mathbf)((.))<br>replace with \1{\2}</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/17/handon-networking/" itemprop="url">
                handon-networking
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-17T09:44:43+02:00" content="2015-10-17">
            2015-10-17
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><ul>
<li><p>start capture:</p>
<ul>
<li><img src="/network/enc_unit.png" alt=""><ul>
<li>from the image we can verify that<ul>
<li>an Ethernet frame[Data link layer] encapsulates an IP datagram(Internet Protocol)[Network layer]. </li>
<li>an IP datagram encapulates a TCP segment(Transmission Contorl protocol)[transport layer]. </li>
<li>a TCP segment encapsulates an HTTP message(<code>hypertext transfer protocol</code>).<ul>
<li>if use UDP then the 4th line is: <code>User Datagram Protocol</code></li>
</ul>
</li>
<li>noted that the <code>Frame</code> on top is nor a real protocol but used by Wireshrk as a base for all the protocols showing some info from capturing</li>
</ul>
</li>
</ul>
</li>
<li><p>filter: <code>HTTP</code>(can also apply TCP…..)</p>
<ul>
<li><p>At the HTTP protocol level exchanged messages are : </p>
<ul>
<li>GET HTTP/1.1 from the web browser to get the content from a web server</li>
<li>HTTP/1.1 responses from the server containing the content of the web site. <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">104</span> <span class="number">4.435759000</span> vpn-<span class="number">252</span>-<span class="number">095</span><span class="class">.epfl</span><span class="class">.ch</span> icvmwebsrv2<span class="class">.epfl</span><span class="class">.ch</span> HTTP    <span class="number">610</span> GET /favicon<span class="class">.ico</span> HTTP/<span class="number">1.1</span> </span><br><span class="line"><span class="number">105</span> <span class="number">4.442593000</span> icvmwebsrv2<span class="class">.epfl</span><span class="class">.ch</span> vpn-<span class="number">252</span>-<span class="number">095</span><span class="class">.epfl</span><span class="class">.ch</span> HTTP    <span class="number">463</span> HTTP/<span class="number">1.1</span> <span class="number">404</span> Not Found  (text/html)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>at the TCP protocol level exchanged messages are: </p>
<ul>
<li>SYN (to initiate a TCP connection), SYN ACK, data packets, FIN (to end a connection) and FIN ACK.<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">9</span>  <span class="number">3.855667000</span> vpn-<span class="number">252</span>-<span class="number">095.</span>epfl.ch icvmwebsrv2.epfl.ch TCP <span class="number">66</span>  <span class="number">52449</span>→http [SYN] Seq=<span class="number">0</span> Win=<span class="number">8192</span> Len=<span class="number">0</span> MSS=<span class="number">1160</span> WS=<span class="number">256</span> SACK_PERM=<span class="number">1</span>`</span><br><span class="line"><span class="number">11</span>  <span class="number">3.861081000</span> icvmwebsrv2.epfl.ch vpn-<span class="number">252</span>-<span class="number">095.</span>epfl.ch TCP <span class="number">66</span>  http→<span class="number">52449</span> [SYN, ACK] Seq=<span class="number">0</span> Ack=<span class="number">1</span> Win=<span class="number">14600</span> Len=<span class="number">0</span> MSS=<span class="number">1160</span> SACK_PERM=<span class="number">1</span> WS=<span class="number">128</span></span><br><span class="line"><span class="number">12</span>  <span class="number">3.861318000</span> vpn-<span class="number">252</span>-<span class="number">095.</span>epfl.ch icvmwebsrv2.epfl.ch TCP <span class="number">54</span>  <span class="number">52449</span>→http [ACK] Seq=<span class="number">1</span> Ack=<span class="number">1</span> Win=<span class="number">16384</span> Len=<span class="number">0</span></span><br><span class="line"><span class="number">114</span> <span class="number">14.021210000</span>    vpn-<span class="number">252</span>-<span class="number">095.</span>epfl.ch icvmwebsrv2.epfl.ch TCP <span class="number">54</span>  <span class="number">52449</span>→http [FIN, ACK] Seq=<span class="number">1211</span> Ack=<span class="number">8243</span> Win=<span class="number">16384</span> Len=<span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li></li>
</ul>
</li>
<li>set HTTP filter:<ul>
<li>analyze one HTTP message: the encapsulation of HTTP protocol data units<ul>
<li><img src="/network/enc_unit2.png" alt=""></li>
<li>num of bytes the HTTP message contain: <ul>
<li>: Transmission Control Portocol - TCP Segment Len: 287</li>
</ul>
</li>
</ul>
<ul>
<li>num of bytes TCP and IP headers together add to the HTTP message<ul>
<li>: Internet Prorocol - Header length: 20 bytes</li>
<li>: TCP - Header length: 32 bytes</li>
<li>intotal: 52 bytes</li>
</ul>
</li>
</ul>
<ul>
<li>Ethernet frame size<ul>
<li>Frame - frame length: 353 bytes</li>
</ul>
</li>
<li>IP packet size = length of HTTP message data + TCP header + IP header<ul>
<li>IP - total length: 339 bytes</li>
<li>note that Ethernet frame encapsulates the IP packet, which is  = IP packet + Ethernet header, so it is always larger than IP packet size</li>
</ul>
</li>
<li>num of HTTP sessions were established when the Web page was loaded? Check on and # servers the content of the web page is stored<ul>
<li>http filter -&gt; Statistics -&gt; Conversation -&gt; Limit to disply filter</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="EXERCISE_1">EXERCISE 1</h2><p><code>nslookup</code>, <code>dig</code>, and <code>host</code><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// get the server and address of a website</span></span><br><span class="line"><span class="preprocessor"># nslookup www.google.com</span></span><br><span class="line"><span class="comment">// The address 127.0.0.1 is always the IP address of every computer.</span></span><br><span class="line">xizeng@icivrgsrv1:~$ nslookup localhost</span><br><span class="line">Server:         <span class="number">128.178</span><span class="number">.15</span><span class="number">.7</span></span><br><span class="line">Address:        <span class="number">128.178</span><span class="number">.15</span><span class="number">.7</span><span class="preprocessor">#<span class="number">53</span></span></span><br><span class="line"></span><br><span class="line">Name:   localhost.epfl.ch</span><br><span class="line">Address: <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line">$ ifconfig</span><br><span class="line"><span class="comment">// get IP address and interface(ethernet, local loopback) of my </span></span><br><span class="line"><span class="comment">// tation</span></span><br><span class="line"></span><br><span class="line">$ ping &lt;url&gt;</span><br><span class="line"><span class="comment">// check whether the following machine is reachable</span></span><br><span class="line"><span class="comment">// check minimum, average, and maximum round trip time</span></span><br><span class="line"></span><br><span class="line">$ traceroute &lt;url&gt;</span><br><span class="line"><span class="comment">// check the routers between my station and host of &lt;url&gt;</span></span><br><span class="line"><span class="comment">// RETURN RTT: round trip time</span></span><br><span class="line"><span class="comment">// ping returns as output whether a destination is reachable </span></span><br><span class="line"><span class="comment">// along with statistics about packet loss and RTT, traceroute </span></span><br><span class="line"><span class="comment">// shows all the hops in the routing path from source to </span></span><br><span class="line"><span class="comment">// destination along with RTT results for each hop.</span></span><br><span class="line"></span><br><span class="line">$ xizeng@icivrgsrv1:~$ grep <span class="string">"ssh"</span> /etc/services</span><br><span class="line">ssh             <span class="number">22</span>/tcp                          <span class="preprocessor"># SSH Remote Login Protocol</span></span><br><span class="line">ssh             <span class="number">22</span>/udp</span><br><span class="line"></span><br><span class="line">$ netstat -t</span><br><span class="line"><span class="comment">// TCP connections that are up and running on your system</span></span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="Exercise_2：">Exercise 2：</h2><ul>
<li><code>traceroute</code> to different destination, can find that all of the connection diverge at perticular router<ul>
<li>if in EPFL network, than this special router is the one connects EPFL and SWITCH, and for the trace of the route, the next router should belong ro SEITCH education network</li>
<li>also notes that the number of hops is not proportional to the physical distance.</li>
<li>if traceroute from A to B and then from B to A, we can verify that<ul>
<li>routes on the Internet do not need to be symmetric<ul>
<li>the forward path and reverse path do not pass the same router<ul>
<li>may choose seperate routers for ingoing and outgoing connections for load balancing</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>for the common routers between the forward and the reverse path, the IP adress may be different: <ul>
<li>both IP addresses indeed belong to the same router, but have been allocated to different interfaces of it</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>if we for each packet size = [50,150,500,1000,1500], we send 50 packet by <code>ping</code> to www.google.com and plot it:<ul>
<li>for same size of packet, delay randomly varies over time. This is mostly due to the variability of processing and queuing delays. The degree of variability is related to the quality of the connection, and it does not necessarily depend on the physical distance or even the number of hops.</li>
<li>for different size, delay may or may not depends on the size of the ping packets for all destinations<ul>
<li>if the transmission delay is the major factor in the overall delay, the dependence is strong</li>
<li>if not, it is weak:<ul>
<li>the destination is far away, and therefore the propagation delay (independent from the packet size) is significantly higher than the transmission delay (dependent on the packet size) </li>
<li>queueing delay (rather than the transmission delay) is the one to have the most significant part in the total delay sur to the high congestion in the network (high congestion can explain the “unexpected” drops as the size increase)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>analyze of delay<ul>
<li>estimation of the delay:<ul>
<li>given </li>
<li>www.google.com_avg.txt : (for each packet size send 50 packets)</li>
<li>packet size/ minimumu delay/ max delay(for a packet in # packet size)<br>  50 4.638 <em>4.386</em><br>  250 4.700 4.480<br>  500 4.764 4.574<br>  750 4.861 4.776<br>  1000 4.937 4.810<br>  1250 5.211 4.963<br>  1500 5.237 <em>5.056</em></li>
</ul>
</li>
</ul>
</li>
<li>The propagation delay <ul>
<li>does not depend on the packet size. It’s related to the link and, in general, does not vary (except if the link varies: cable, satellite, etc.) </li>
</ul>
</li>
<li>The queuing delay <ul>
<li>only depends on the congestion of the network. It will increase with the amount of traffic on the network (the more traffic there is, the more our packet will wait to be treated at a router).</li>
<li>avarage ~= average difference between the average and the minimum delays, taking into account all the packet sizes, for this example it is: 1/7 * ( (4.638-4.386) + (4.700-4.480) + (4.764-4.574) + (4.861-4.776) + (4.937-4.810) + (5.211-4.963) + (5.237-5.056) ) = 0.1861 [ms]</li>
</ul>
</li>
<li>The transmission delay <ul>
<li>almost proportional to the packet size; for a fixed packet size, it is constant.</li>
<li>assume that the other delay for the mimi delay of different packet size is the same; </li>
<li>~= difference between the minimum delays for the largest packet and the smallest packet: 5.056 - 4.386 = 0.67 [ms] for 1500-50 = 1450 byte packets, 1 Byte = 8 bits<ul>
<li><code>link throughput</code> = 8<em>1450/[0.67</em>10^(-3)] Mbps or Bit/s</li>
</ul>
</li>
</ul>
</li>
<li>The processing delay <ul>
<li>can depend on the packet size, but to a much smaller degree than transmission delay; for a fixed packet size, it is reasonably constant.</li>
<li>for packet in same size, assume the transmission+propagetion+processing delay is the same</li>
<li>process + propage dalay ~= minimum delay among all minimum delays (usually it is the min delay for the smallest packet) = 4.386 ms</li>
</ul>
</li>
</ul>
<hr>
<p><em>telnet</em></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">telnet&gt; o www.google.ch <span class="number">80</span></span><br><span class="line">Trying <span class="number">173.194</span><span class="number">.112</span><span class="number">.87</span>...</span><br><span class="line">Connected to www.google.ch.</span><br><span class="line">Escape character is <span class="string">'^]'</span>.</span><br><span class="line">GET <span class="regexp">/ HTTP/</span><span class="number">1.1</span></span><br><span class="line"><span class="string">host:</span> www.google.ch</span><br><span class="line"></span><br><span class="line"><span class="comment">// below is output </span></span><br><span class="line">HTTP/<span class="number">1.1</span> <span class="number">200</span> OK</span><br><span class="line"><span class="string">Date:</span> Sat, <span class="number">17</span> Oct <span class="number">2015</span> <span class="number">16</span>:<span class="number">56</span>:<span class="number">32</span> GMT</span><br><span class="line"><span class="string">Expires:</span> -<span class="number">1</span></span><br><span class="line">Cache-<span class="string">Control:</span> <span class="keyword">private</span>, max-age=<span class="number">0</span></span><br><span class="line">Content-<span class="string">Type:</span> text/html; charset=ISO-<span class="number">8859</span>-<span class="number">1</span></span><br><span class="line"><span class="string">P3P:</span> CP=<span class="string">"This is not a P3P policy! See http://www.google.com/support/accounts/bin/answer.py?hl=en&amp;answer=151657 for more info."</span></span><br><span class="line"><span class="string">Server:</span> gws</span><br><span class="line">X-XSS-<span class="string">Protection:</span> <span class="number">1</span>; mode=block</span><br><span class="line">X-Frame-<span class="string">Options:</span> SAMEORIGIN</span><br><span class="line"></span><br><span class="line"><span class="comment">// two cookie is stored by www.google.ch</span></span><br><span class="line">Set-<span class="string">Cookie:</span> PREF=ID=<span class="number">1111111111111111</span>:FF=<span class="number">0</span>:TM=<span class="number">1445100992</span>:LM=<span class="number">1445100992</span>:V=<span class="number">1</span>:S=<span class="number">1</span>eJR j9kphnaCmoih; expires=Thu, <span class="number">31</span>-Dec-<span class="number">2015</span> <span class="number">16</span>:<span class="number">02</span>:<span class="number">17</span> GMT; path=/; domain=.google.ch</span><br><span class="line"></span><br><span class="line">Set-<span class="string">Cookie:</span> NID=<span class="number">72</span>=v6MxR_2MLSDmKT0EyV6qrIddaWKZ6FdL-zOEhz-DoRiCqqi8w8u8QBdpq-</span><br><span class="line">fa6cpkDGAmqTiIQYOhSVNsy01hrc3cBzx3C7Ef9KzArtRCiwX-pNIdhMIyMe8bpPAt7rDGLagPsCP4; expires=Sun, <span class="number">17</span>-Apr-<span class="number">2016</span> <span class="number">16</span>:<span class="number">56</span>:<span class="number">32</span> GMT; path=/; domain=.google.ch; HttpOnly</span><br><span class="line"></span><br><span class="line">Accept-<span class="string">Ranges:</span> none</span><br><span class="line"><span class="string">Vary:</span> Accept-Encoding</span><br><span class="line">Transfer-<span class="string">Encoding:</span> chunked</span><br></pre></td></tr></table></figure>
<hr>
<p><em>simple email client: SMTP</em><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ grep smtp /etc/services</span><br><span class="line">PORT <span class="number">25</span></span><br><span class="line"><span class="comment">// SMTP use port 25</span></span><br><span class="line"></span><br><span class="line">$ telnet test.smtp.org <span class="number">25</span></span><br><span class="line"><span class="comment">// test SMTP server </span></span><br><span class="line"></span><br><span class="line">$ help</span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span> This is sendmail version <span class="number">8.15</span><span class="number">.2</span></span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span> Topics:</span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span>       HELO    EHLO    MAIL    RCPT    DATA</span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span>       RSET    NOOP    QUIT    HELP    VRFY</span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span>       EXPN    VERB    ETRN    DSN     AUTH</span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span>       STARTTLS</span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span> For more info use <span class="string">"HELP &lt;topic&gt;"</span>.</span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span> To report bugs in the implementation see</span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span>       http:<span class="comment">//www.sendmail.org/email-addresses.html</span></span><br><span class="line"><span class="number">214</span>-<span class="number">2.0</span><span class="number">.0</span> For local information send email to Postmaster at your site.</span><br><span class="line"><span class="number">214</span> <span class="number">2.0</span><span class="number">.0</span> End of HELP info</span><br><span class="line"><span class="comment">// check if the email addresses are vaild</span></span><br><span class="line">$ VRFY admin@test.smtp.org</span><br><span class="line"><span class="number">553</span> <span class="number">5.3</span><span class="number">.0</span> admin@test.smtp.org... No such user</span><br><span class="line"></span><br><span class="line">$ VRFY bouncer@test.smtp.org</span><br><span class="line"><span class="number">553</span> <span class="number">5.3</span><span class="number">.0</span> bouncer@test.smtp.org... Disabled due to abuse</span><br><span class="line"></span><br><span class="line">$ VERY bit-bucket@test.smtp.org</span><br><span class="line"><span class="number">500</span> <span class="number">5.5</span><span class="number">.1</span> Command unrecognized: <span class="string">"VERY bit-bucket@test.smtp.org"</span></span><br><span class="line">VRFY bit-bucket@test.smtp.org</span><br><span class="line"><span class="number">250</span> <span class="number">2.1</span><span class="number">.5</span> &lt;bit-bucket@test.smtp.org&gt;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">xizeng@icivrgsrv1:~$ telnet mail<span class="class">.epfl</span><span class="class">.ch</span> <span class="number">25</span></span><br><span class="line">Trying <span class="number">128.178</span>.<span class="number">222.71</span>...</span><br><span class="line">Connected to mail<span class="class">.epfl</span><span class="class">.ch</span>.</span><br><span class="line">Escape character is <span class="string">'^]'</span>.</span><br><span class="line"><span class="number">220</span> mail<span class="class">.epfl</span><span class="class">.ch</span> AngelmatoPhylax SMTP proxy</span><br><span class="line">$ helo</span><br><span class="line"><span class="number">250</span> mail<span class="class">.epfl</span><span class="class">.ch</span></span><br><span class="line">$ mail from: &lt;anyname@gmail.com&gt;</span><br><span class="line">...</span><br><span class="line">$ rcpt to: &lt;firstname.lastname@epfl.ch&gt;</span><br><span class="line"><span class="number">503</span> successful MAIL needed before RCPT</span><br><span class="line">$ data</span><br><span class="line"><span class="number">503</span> successful RCPT needed before DATA</span><br><span class="line">$ subject: hhhhh</span><br><span class="line">write something here!!!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">.</span><br><span class="line"><span class="comment">// end</span></span><br><span class="line">$ quit</span><br><span class="line"><span class="number">221</span> bye bye</span><br><span class="line">Connection closed by foreign host.</span><br></pre></td></tr></table></figure>
<p>And then you will receive the email!<br>notice that only ordered instruction will be recognized</p>
<hr>
<p><em>use firefox to track the network</em><br>!()[/network/firefox.png]</p>
<ul>
<li>info:</li>
<li>application protocol: HTTP 1.1</li>
<li>content type: audio/mpeg</li>
<li>contene size: 8315297 bytes</li>
<li>transport protocol: TCP</li>
</ul>
<hr>
<p>by the way, the website <a href="https://soundcloud.com/relaxdaily/instrumental-music-to-relax" target="_blank" rel="external">https://soundcloud.com/relaxdaily/instrumental-music-to-relax</a> is really good!</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">&raquo;</a>
  </nav>

 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/xx.jpg" alt="XXXH" itemprop="image"/>
          <p class="site-author-name" itemprop="name">XXXH</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">11</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">0</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">12</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XXXH</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
