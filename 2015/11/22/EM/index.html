<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>




  <meta name="keywords" content="EM,GMM,clustering,guassian,machine learning,unsupervised learning," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="when we try to approace by MLE for optimation of the marginal distribution x. which is difficult to solve &amp;amp; x in a joint distribution with r (from graphic model: summing joint dist(x, y) get margi">
<meta property="og:type" content="article">
<meta property="og:title" content="EM in GMM and in general">
<meta property="og:url" content="http://yoursite.com/2015/11/22/EM/index.html">
<meta property="og:site_name" content="XXXH">
<meta property="og:description" content="when we try to approace by MLE for optimation of the marginal distribution x. which is difficult to solve &amp;amp; x in a joint distribution with r (from graphic model: summing joint dist(x, y) get margi">
<meta property="og:image" content="http://yoursite.com/./ml/gmm_1.png">
<meta property="og:image" content="http://yoursite.com/./ml/gmm_2.png">
<meta property="og:updated_time" content="2015-11-29T06:29:18.450Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EM in GMM and in general">
<meta name="twitter:description" content="when we try to approace by MLE for optimation of the marginal distribution x. which is difficult to solve &amp;amp; x in a joint distribution with r (from graphic model: summing joint dist(x, y) get margi">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>



  <title> EM in GMM and in general | XXXH </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">XXXH</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            Tags
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              EM in GMM and in general
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-11-22T21:44:43+01:00" content="2015-11-22">
            2015-11-22
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; In
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/machine-learning/" itemprop="url" rel="index">
                  <span itemprop="name">machine learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><p>when we try to approace by MLE for optimation of the marginal distribution x. which is difficult to solve &amp; x in a joint distribution with r (from graphic model: summing joint dist(x, y) get marginal dist). then we trun to optimaze the joing distribution. and we try to marginalize/kick out r (consider as latent variable) out and make our life easier<br>to solve the problem we need to use EM as solver.<br>my implementaion of EM algo: </p>
<h2 id="<!-more->"><a id="more"></a></h2><p><img src="./ml/gmm_1.png" alt=""><br><img src="./ml/gmm_2.png" alt=""></p>
<h1 id="k_means_-_online_learning">k means - online learning</h1><h3 id="define_cost:_‘distortion_measure’:">define cost: ‘distortion measure’:</h3><p>for each $x_n$, cal $dist(x_n,rn)$, sum up to be $J$<br>(r_n indicate the groupk x_n assighed to)<br>$$J = \sum_n \sum<em>k r</em>{nk} |x_n - \mu_k|^2$$<br>goal: given k, xn, minimize J w.r.t parameter $\mu_k$ and $r_nk$<br>since that they are depend to each other, need to optimatize in two step, ie. no close form<br>algo:<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">randomly initialize <span class="variable">$&#123;</span>\mu_k&#125;<span class="variable">$,</span> assigh rnk <span class="keyword">for</span> each point, by minimize dist(x_n, rn)</span><br><span class="line">having rn <span class="keyword">for</span> all points, minimize <span class="constant">J </span>by take derivative wrt <span class="variable">$\</span>mu_k<span class="variable">$ </span><span class="keyword">and</span> update <span class="variable">$\</span>mu_k<span class="variable">$</span><br><span class="line"></span>repeat</span><br></pre></td></tr></table></figure></p>
<h1 id="GMM:">GMM:</h1><h2 id="START_FROM_GRAPHIC_MODEL">START FROM GRAPHIC MODEL</h2><p>binary variable {r_k} generated from <strong>multinomial distribution $Mult(r|\pi)$ $\pi &amp; r \in R ^k$ is a vector here</strong> and assume p(x|r_k) is gaussian distribution<br>by graphic model, sum of joint distribution x,r for all possible state of latent variable r gives the marginal distribution of x. to maximize the marginal distribution of x, we can instead maximize the sum of( product of( the joing distribution x,r with marginal distribution of r)) – which define our cost function, (in this case, we say that we marginalize the latent variable out!)</p>
<p>as usual, we minimize the cost function, in this case, we have three parameters are they are depent on each other, no close form can derived, we use EM algo to find the solution.</p>
<p>the general idea is that, we first (random) initialize two of them, and then cal the third one, then cal another one by fix two again.</p>
<p>notice that EM can not only apply on GMM and in this way the problem is not identifiable, ie, the ‘almost best’ solution is not unique</p>
<h2 id="relation_to_k_means">relation to k means</h2><p>we can find that using GMM with EM is very similar to the kmeans algorithm and actually, it can be derived that when the covalance matrix of the gmm is approacing 0*I, GMM is indeed kmeans.</p>
<h2 id="attention,_dont_mix_up_the_complete_data_likelihhod_and_the_imcomplete_data(x)_likelihood">attention, dont mix up the complete data likelihhod and the imcomplete data(x) likelihood</h2><p>complete data likelihhod come from the joing distubtion<br>$$p(X,r) = \prod_n \prod_k \pi<em>k^{r</em>{nk}} N(x_n|\mu_k,\Sigma<em>k)^{r</em>{nk}}$$</p>
<p>and the likelihood which we indeed define as cost function is<br>$$p(X) = \prod_n \sum_k \pi_k N(x_n|\mu_k,\Sigma_k)$$</p>
<h1 id="EM">EM</h1><p>we mention that EM can not only works for GMM, it can apply generally to other MLE form joing distribution.(ie, in this case, p(x|r_k) may not be gaussian distribution)</p>
<p>consider the cost is depend on x(given),z,\theta<br><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">initialize <span class="string">\theta(random)</span></span><br><span class="line"><span class="attribute">E</span>: expectation, since z unknow, taking expection <span class="keyword">and</span> <span class="keyword">then</span> optimate p(z|x,<span class="string">\theta)</span></span><br><span class="line">(like taking mean <span class="keyword">of</span> each cluster <span class="keyword">in</span> kmean)</span><br><span class="line">M : maximization, now we have z, <span class="keyword">by</span> minimizing cost <span class="keyword">function</span> wrt <span class="string">\theta,</span> we can update <span class="string">\theta</span></span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="below_is_some_knowledge_fragments">below is some knowledge fragments</h2><ul>
<li>probability density function $p(X|\mu, \sigma)$<br>intergrate pdf get cdf</li>
</ul>
<h1 id="multivariable_gaussian_distribution">multivariable gaussian distribution</h1><p>pdf:<br>$X-N(\mu, \Sigma)$</p>
<p>$$p(X|\mu, \Sigma) = \frac{1}{(2 \pi)^{2n}* det(\Sigma)^{1/2}} exp(-1/2 (x-\mu)^T \Sigma^{-1} (x-\mu))$$</p>
<p>$$\Sigma = E[(X-\mu)(x-\mu)^T]$$</p>
<ul>
<li>given X, </li>
<li>assume X from $N(\mu, \Sigma)$, estimate $(\mu, \Sigma)$ by MLE:</li>
<li><p>take log, get log likelihood function<br>$$L = ln(p(X|\mu, \Sigma)) = ln(\prod_n p(\mathbf{x_n}|\mu, \Sigma)) = -2/N * ln(|\Sigma|) -1/2 (x-\mu)^T \Sigma^{-1} (x-\mu) + const$$</p>
</li>
<li><p>take derivative wrt $\Sigma$ and $\mu$<br>$$\partial L / \partial \Sigma^{-1} = -2/N * |\Sigma|^{T} - 1/2 N \sum_n (x-\mu)(x-\mu)^T = 0$$<br>get<br>for $\Sigma$<br>$$\Sigma = 1/N \sum_n (x-\mu)(x-\mu)^T$$<br>for $\mu$<br>$$\partial L / \partial \mu = … = 0$$<br>$$\mu = 1/N \sum_n \mathbf{x_n} $$</p>
</li>
</ul>
<h1 id="multi_gaussian_models">multi gaussian models</h1><p>x can only belong to one of the k models - hard clustering</p>
<p>$$p(x|\mu,\Sigma,r) = N(x|\mu_j,\Sigma_j), x \in group j$$</p>
<ul>
<li>compact form</li>
</ul>
<p>$$p(x|\mu,\Sigma,r) = \prod_k [N(x|\mu_k,\Sigma_k)]^{r_k}$$<br>notice that product + rk means only choose one N</p>
<h1 id="Gaussiam_Mixtrue_Distribution">Gaussiam Mixtrue Distribution</h1><p>x have posivility belong to severals groups</p>
<p>$$p(\mathbf{x}) = \sum_k \pi_k N(x|\mu_k,\Sigma_k)$$<br>notice that here sum up all N weighted by $\pi_k$</p>
<p>consider{x1, x2…}</p>
<p>$$p(\mathbf{X}) = \prod_n p(\mathbf{x_n})  = \prod_n \sum_k \pi_k N(x|\mu_k,\Sigma_k)$$</p>
<ul>
<li>take log</li>
<li><p>(1.1)$$L = ln(p(\mathbf{X}) = \prod_n p(\mathbf{x_n})) = ln(\prod_n \sum_k \pi_k N(x|\mu_k,\Sigma_k))$$ </p>
</li>
<li><p>take derivative wrt $\Sigma$ and $\mu$</p>
</li>
</ul>
<h3 id="for_each_$\mu_k$">for each $\mu_k$</h3><p>$$\partial L / \partial \mu_k = \sum_n  \frac{\pi_k N(x|\mu_k,\Sigma_k)}{\sum_k \pi_k N(x|\mu_k,\Sigma_k)} * \partial exp() / \partial \mu_k= 0$$</p>
<p>$$\frac{\pi_k N(x|\mu_k,\Sigma_k)}{\sum_k \pi_k N(x|\mu_k,\Sigma<em>k)} = p</em>{nk}$$</p>
<p>$$\partial exp(..) / \partial \mu_k = \Sigma_k(x_n - \mu_k)$$</p>
<p><strong>TARGET1:——-&gt;&gt;</strong></p>
<p>$$\mu_k = \frac {\sum<em>n (p</em>{nk} x_n)} {\sum<em>n p</em>{nk}}$$</p>
<p>def: $N_k = \sum<em>n {p</em>{nk}} $ : effective # points assigned to cluster k</p>
<p><strong>TARGET2:——-&gt;&gt;</strong></p>
<p>$$\pi_k = N_k/N = \sum<em>n {p</em>{nk}} / N$$</p>
<h3 id="for_each_$\Sigma_k$">for each $\Sigma_k$</h3><p><strong>TARGET3:——-&gt;&gt;</strong></p>
<p>$$\Sigma_k = \frac {\sum<em>n (p</em>{nk} (x-\mu)(x-\mu)^T)}{\sum<em>n p</em>{nk}}$$</p>
<h4 id="bayes_view_point">bayes view point</h4><hr>
<h3 id="marginal_distribution_$p(\mathbf{r})$">marginal distribution $p(\mathbf{r})$</h3><p>$r \in R^k$<br>the kth entry = 1, ie belong to kth group:</p>
<h4 id="prior_probality">prior probality</h4><p>$$p(r) = \pi_k,if: r_k = 1$$ </p>
<p>compactly, notice that only one $r_j = 1$ </p>
<p>$$p(\mathbf{r}) = \prod_k p(r_k = 1)=\prod_k \pi_k^{r_k}$$</p>
<hr>
<h3 id="conditional_distribution_$p(x|r)$">conditional distribution $p(x|r)$</h3><p>know x belong to kth group, the distribution of x<br>$$p(x|r_k=1) = N(x|\mu_k,\Sigma_k)$$<br>compactly:<br>$$p(x|r) = \prod_k N(x|\mu_k,\Sigma_k)^{r_k}$$</p>
<hr>
<h3 id="joint_distribution_$p(x,r)$">joint distribution $p(x,r)$</h3><p>$$p(x,r_k) {：if r_k = 1} = p(x|r_k)p(\mathbf{r}_k) = \prod_k \pi_k N(x|\mu_k,\Sigma_k) $$<br>compactly<br> $$p(x,r) = p(x|r)p(\mathbf{r}) = \prod_k \pi_k^{r_k} N(x|\mu_k,\Sigma_k)^{r_k} $$</p>
<hr>
<h3 id="marginal_distribution_of_x_$p(x)$">marginal distribution of x $p(x)$</h3><p>= sum up joint distribution over all possible states of r(r have k state)<br>$$p(x) = \sum_r p(x,r) = \sum_k p(x,r_k=1) =\sum_k p(x|r_k=1)p(\mathbf{r_k}=1) $$</p>
<p>$$= \sum_k p(x|r_k=1)\pi_k^{r_k} = \sum_k N(x|\mu_k,\Sigma_k)\pi_k^{r_k}$$<br>since k is specified<br>$$= \sum_k N(x|\mu_k,\Sigma_k)\pi_k$$ </p>
<p>consider N points:<br>$$p(X) = \prod_n p(x_n) = \prod_n \sum_k N(x|\mu_k,\Sigma_k)\pi_k$$<br>which is the same as (1,1)<br>take log, get the loglikehood function, take derivative then </p>
<ul>
<li>It follows that for every observed data point $x_n$ there is a corresponding latent vector $r_n$ , i.e., its sub-class</li>
</ul>
<hr>
<h4 id="posterior_probability_$p(rk=1|x)_=_p{nk}_=_\gamma(r_k)$">posterior probability $p(r<em>k=1|x) = p</em>{nk} = \gamma(r_k)$</h4><p>responsibility<br>$$p(r_k=1|x) = \frac{p(r_k=1)p(x|r_k=1)}{\sum_j p(r_i=1)p(x|r_j=1) }=\frac{\pi_k N(x|\mu_k,\Sigma_k)}{\sum_k \pi_k N(x|\mu_k,\Sigma<em>k)} = p</em>{nk} = \gamma(r_k)$$</p>
<hr>
<h2 id="generate_MLE_problem_for_GMM_modles">generate MLE problem for GMM modles</h2><p>We are given N i.i.d. samples ${x_n}$ with unknown latent points ${r_n}$(the addigh of the point) with parameters $\pi_k$.<br>The samples have parameters: mean $\mu$ and covariance $\Sigma$</p>
<p>Goal is to estimate the three sets of parameters :<br> ${x_n}$<br> ${r_n}$<br> mean $\mu$ and covariance $\Sigma$</p>
<hr>
<h3 id="notes_on_how_to_cal_the_gradient_and_the_minimum:">notes on how to cal the gradient and the minimum:</h3><p>pre:</p>
<ol>
<li>$$log(\sum_k a_k b_k)&gt;=\sum_k a_k log(b_k)$$</li>
<li>$\Sigma$ symmetry</li>
</ol>
<hr>
<h3 id="problem_of_GMM">problem of GMM</h3><ol>
<li>if distribution one of the cluster have vary small variance, then the values are close to mean, therefore the exponent term will goes to 0 and then exp() -&gt; 1, while 1/var will goes to infinity, then the equation which we want to find macimum will goes to infinity</li>
</ol>
<ul>
<li>solution: resetting mean or variance</li>
<li><a href="http://www.cedar.buffalo.edu/~srihari/CSE574/Chap9/Ch9.2-MixturesofGaussians.pdf" target="_blank" rel="external">http://www.cedar.buffalo.edu/~srihari/CSE574/Chap9/Ch9.2-MixturesofGaussians.pdf</a></li>
</ul>
<ol>
<li><p>not identifiable<br>就是给三个组套上{1,2,3}的安排， 组名的顺序变换之后分类的结果都是一样好的，有k组实际上就是有2^k个solution</p>
</li>
<li><p>Takes many more iterations than K-means</p>
</li>
<li>EM is not guaranteed to find global maximum of log likelihood function </li>
</ol>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/EM/" rel="tag">#EM</a>
          
            <a href="/tags/GMM/" rel="tag">#GMM</a>
          
            <a href="/tags/clustering/" rel="tag">#clustering</a>
          
            <a href="/tags/guassian/" rel="tag">#guassian</a>
          
            <a href="/tags/machine-learning/" rel="tag">#machine learning</a>
          
            <a href="/tags/unsupervised-learning/" rel="tag">#unsupervised learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/11/25/kernel method/" rel="prev">kernel method</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/11/15/install-lua/" rel="next">install lua</a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/xx.jpg" alt="XXXH" itemprop="image"/>
          <p class="site-author-name" itemprop="name">XXXH</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">28</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">7</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">37</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ZENGXH" target="_blank">github</a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/xiao-xiao-hui-49" target="_blank">zhihu</a>
              </span>
            
              <span class="links-of-author-item">
                <a href="4650xh@gmail.com" target="_blank">email</a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#<!-more->"><span class="nav-number">1.</span> <span class="nav-text"></span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#k_means_-_online_learning"><span class="nav-number"></span> <span class="nav-text">k means - online learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#define_cost:_‘distortion_measure’:"><span class="nav-number">0.1.</span> <span class="nav-text">define cost: ‘distortion measure’:</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GMM:"><span class="nav-number"></span> <span class="nav-text">GMM:</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#START_FROM_GRAPHIC_MODEL"><span class="nav-number">1.</span> <span class="nav-text">START FROM GRAPHIC MODEL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#relation_to_k_means"><span class="nav-number">2.</span> <span class="nav-text">relation to k means</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention,_dont_mix_up_the_complete_data_likelihhod_and_the_imcomplete_data(x)_likelihood"><span class="nav-number">3.</span> <span class="nav-text">attention, dont mix up the complete data likelihhod and the imcomplete data(x) likelihood</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EM"><span class="nav-number"></span> <span class="nav-text">EM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#below_is_some_knowledge_fragments"><span class="nav-number">1.</span> <span class="nav-text">below is some knowledge fragments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#multivariable_gaussian_distribution"><span class="nav-number"></span> <span class="nav-text">multivariable gaussian distribution</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#multi_gaussian_models"><span class="nav-number"></span> <span class="nav-text">multi gaussian models</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Gaussiam_Mixtrue_Distribution"><span class="nav-number"></span> <span class="nav-text">Gaussiam Mixtrue Distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#for_each_$\mu_k$"><span class="nav-number">0.1.</span> <span class="nav-text">for each $\mu_k$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#for_each_$\Sigma_k$"><span class="nav-number">0.2.</span> <span class="nav-text">for each $\Sigma_k$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bayes_view_point"><span class="nav-number">0.2.1.</span> <span class="nav-text">bayes view point</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#marginal_distribution_$p(\mathbf{r})$"><span class="nav-number">0.3.</span> <span class="nav-text">marginal distribution $p(\mathbf{r})$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#prior_probality"><span class="nav-number">0.3.1.</span> <span class="nav-text">prior probality</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conditional_distribution_$p(x|r)$"><span class="nav-number">0.4.</span> <span class="nav-text">conditional distribution $p(x|r)$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#joint_distribution_$p(x,r)$"><span class="nav-number">0.5.</span> <span class="nav-text">joint distribution $p(x,r)$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#marginal_distribution_of_x_$p(x)$"><span class="nav-number">0.6.</span> <span class="nav-text">marginal distribution of x $p(x)$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#posterior_probability_$p(rk=1|x)_=_p{nk}_=_\gamma(r_k)$"><span class="nav-number">0.6.1.</span> <span class="nav-text">posterior probability $p(rk=1|x) = p{nk} = \gamma(r_k)$</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generate_MLE_problem_for_GMM_modles"><span class="nav-number">1.</span> <span class="nav-text">generate MLE problem for GMM modles</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#notes_on_how_to_cal_the_gradient_and_the_minimum:"><span class="nav-number">1.1.</span> <span class="nav-text">notes on how to cal the gradient and the minimum:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#problem_of_GMM"><span class="nav-number">1.2.</span> <span class="nav-text">problem of GMM</span></a></li></ol></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XXXH</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.1" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
