<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>




  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="for input may be preprocessed
- like fixed size of the image
- also called feature extraction
- speed up computation
- easier to solve problem

supervised learning
classification- discrete output
gene">
<meta property="og:type" content="article">
<meta property="og:title" content="prml">
<meta property="og:url" content="http://yoursite.com/2015/10/18/prml/index.html">
<meta property="og:site_name" content="XXXH">
<meta property="og:description" content="for input may be preprocessed
- like fixed size of the image
- also called feature extraction
- speed up computation
- easier to solve problem

supervised learning
classification- discrete output
gene">
<meta property="og:updated_time" content="2015-10-18T17:47:48.365Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="prml">
<meta name="twitter:description" content="for input may be preprocessed
- like fixed size of the image
- also called feature extraction
- speed up computation
- easier to solve problem

supervised learning
classification- discrete output
gene">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>



  <title> prml | XXXH </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">XXXH</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            Tags
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              prml
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-18T19:47:48+02:00" content="2015-10-18">
            2015-10-18
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><p>for input may be <em>preprocessed</em></p>
<pre><code>-<span class="ruby"> like fixed size of the image
</span>-<span class="ruby"> also called feature extraction
</span>-<span class="ruby"> speed up computation
</span>-<span class="ruby"> easier to solve problem</span>
</code></pre><ul>
<li>supervised learning<ul>
<li>classification- discrete output<ul>
<li>generalization:= make predictions on novel input<ul>
<li>generalization error = expected value of the misclassification rate when averaged over future data, can never know but can be approximate by:  misclassification rate on a large independenttest set</li>
</ul>
</li>
</ul>
</li>
<li>regression- continues output</li>
</ul>
</li>
<li>unsupervised learning<ul>
<li>goal: knowledge discovery or discover interesting sreuctre in data in murphy</li>
</ul>
<ul>
<li>visualization</li>
<li>clustering</li>
<li>density estimation<br>…</li>
</ul>
</li>
</ul>
<p>#reinforcement learning#</p>
<pre><code>- find suirable actions <span class="keyword">to</span> <span class="keyword">take</span> <span class="keyword">in</span> a given stituation <span class="keyword">in</span> <span class="keyword">order</span> <span class="keyword">to</span> maximized the reward
</code></pre><ul>
<li>function which is linear in unknow parameters are called linear model<ul>
<li>for the polynomial model: $y(x, \mathbf{\beta}) = \beta^0 + \beta_1 x + \beta_2 x^2 + …. + \beta_M x^M$ is <em>nonliear</em> in input x but linear in unknow parameter $\beta$</li>
<li>implement issue:<ul>
<li>choose the value of coefficents/parameters/weights is determined by fitting data - minimizing error function - <em>cost function</em></li>
<li>choose the order M - model selection/comparision<ul>
<li>overfitting</li>
<li>FACT: as M increases, the magnitude of the coefficients typically gets larger</li>
<li>regulazation to fight overfitting<ul>
<li>involves adding a penalty term to the error function (1.2) in order to discourage the coefficients from reaching large values</li>
<li>add $\lambda / 2 | \mathbf{\beta} |^2$ $\lambda$ is the importance of the regulazation term; <ul>
<li>if use quadric regularizer - call <em>ridge regression</em></li>
</ul>
</li>
<li><em>weight decay?</em> in neural network </li>
<li>$\lambda$ can suppressed over-fitting, reduce value of weight, but if $\lambda$ too large, weight goes to 0, will lead to poor fit</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>#probability theory#</p>
<ul>
<li>提出probability 是想更加科学一些</li>
<li><p><em>expectation</em> </p>
<ul>
<li>def: weighted averages of funtions</li>
<li>if we are given a finite number N of points drawn from the probability distribution or probability density<ul>
<li>$E[f] ~- 1/N sum_{n=1,2…N}f(x_n)$       </li>
</ul>
</li>
<li>consider expectation of functions of several variables eg f(x,y)<ul>
<li>expectation have subscript with repesct to different variable:</li>
<li>$E_x[f(x,y)]$ and $E_y[f(x,y)]$</li>
</ul>
</li>
</ul>
</li>
<li><p>variance: $Var[x] = E[x^2] - E[x]^2$</p>
<ul>
<li>consider functions of several variables eg f(x,y)<ul>
<li><em>covariance</em>: $cov[x,y] = E_{x,y}[{x - E[x]}{y^T - E[y^T]}]$  </li>
</ul>
</li>
</ul>
</li>
</ul>
<p>#interpretation of probabilities#</p>
<p>##frequentist interpretation##</p>
<ul>
<li>popular: <em>classical or frequentist way</em><ul>
<li>the probability P of an uncertain event A, P(A) is defined by the frequency of that event based on previous observations</li>
</ul>
</li>
</ul>
<p>##Bayesian interpretation##</p>
<ul>
<li>another point of view: <em>Bayesian view</em> - probability provide a quanrification of uncertainty<ul>
<li>ads: can be used to model our uncertainty of the event without long term frequency</li>
<li>for future event, we do not have historical database thus can not count the frequency.</li>
<li>but can measure the belief in a statement $a$ based on some ‘knowledge’, denote as P(a|K), different K can generate different P(a|K) and even same K can have different P(a|K) – the belief is subjective </li>
</ul>
</li>
<li><em>Bayes rule</em><ul>
<li>consider conditional probabilities </li>
<li>$P(A|B) = P(B|A) P(A)/P(B)$<ul>
<li>interpretation: updating our belief about a hypothesis A in the light of new evidence B<ul>
<li>in likelihood, it is, output brief of y/A given B/input values+paramters </li>
</ul>
</li>
<li>P(A|B): posterior belief</li>
<li>P(A): prior belief</li>
<li>P(B|A): likelihood, ie the B(our model) will occur if A(the output value of the sample data) is true.</li>
<li>P(B) is computed by: $sum_{i=1,2,…}P(B|A_i)P(A_i)$ by <em>marginalisation</em>.</li>
<li>if you still remember the ‘cancer-test examples’ in statistic course, then p(A) is p(cancer), p(B|A) is p(positive|cancer), p(A|B) is p(cancer|positive) which is the value patient care for(but in fact we dont know) </li>
</ul>
</li>
<li>in machine learning, Bayes theorem is used to convert a priot ptobability $P(A) = P(\mathbf{\beta})$ into a porterior probability $P(A|B) = P(\mathbf{\beta}|y)$ by incorpoating the evidence provided by the observed data   <ul>
<li>for $\mathbf{\beta}$ in the polynormial curve fitting model, we can take an approach with Bayes theorem:</li>
<li>$P(\mathbf{\beta} | \mathbf{y}) =\frac{ P(\mathbf{y}|\mathbf{\beta}) p(\mathbf{\beta})}{P(\mathbf{y})} $<ul>
<li>given data {y_1,y_2,…}, we want to know the $\beta$, cant get directly. $P(\mathbf{\beta} | \mathbf{y})$:= <em>posterior probability</em></li>
<li>$P(\beta)$:= prior probability; our assumption of $\beta$</li>
<li>${P(\mathbf{y})}$:= normalization constant since the given data is fixed</li>
<li>$P(\mathbf{y}|\mathbf{\beta})$:= <em>likelihood function</em>;<ul>
<li>can be view as function of parameter $\beta$ </li>
<li>not a probability distrubution, so intergral w.r.t $\beta$ not nessary = 1</li>
</ul>
</li>
<li>state Bayes theorem as : posterior 8 likelihood × prior, consider all of these as function of parameters $\beta$</li>
<li>intergrate both side base on $\beta$: $p(y)= \intergral p(y|\beta)p(\beta)d\beta$</li>
<li>issue: particularly the need to marginalize (sum or integrate) over the whole of parameter space</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>#different view of likelihood function#</p>
<ul>
<li>likelihood function: $P(\mathbf{y}|\mathbf{\beta})$</li>
<li>from frequentist way of interpretation:<ul>
<li>parameter $\beta$ is a fixed parameter, the value is determined by ‘estimator’</li>
<li>A widely used frequentist estimator is <em>maximum likelihood</em>, in which wis set to the value that maximizes the likelihood function </li>
<li>ie. choosing $\beta$ s.t. probability of the observed data is maximized</li>
<li>in practice, use negative log of likelihood function = log-likelihood:= error function(monotonically decreasing)</li>
<li><strong>One approach to determining frequentist error bars is the bootstrap</strong>, <ul>
<li>s1: 就是在已有的dataset(size N)里面random弄出L个dataset(size N) by drawing data from 已有的dataset(抽取方式是，可以重复抽，可以有的没有被抽中)</li>
<li>s2: looking at the variability of predictions between the different bootstrap data sets. then evaluate the accuracy of the estimates of the parameter</li>
</ul>
</li>
<li>drawback: may lead to extreme conclusion if the dataset is bad, eg, a fair-looking coin is tossed three times and lands heads each time. in this case, we will generate parameter $\beta$ to make P(lands head) = 1   </li>
</ul>
</li>
<li>from Bayesian viewpoint:<ul>
<li></li>
</ul>
</li>
</ul>
<p>#Murphy’s explanation – MAP estimate#</p>
<ul>
<li>look at probability is conditional on the test input x, as well as the training set D, and the form of model M that we use to make predictions</li>
<li>Given a probabilistic output, we can always compute our “best guess” as to the “true label” using: </li>
<li>$\hat y = \hat f(\mathbf{x} = argmax_{c=1..C} p(y = c|\mathbf{x},D,M)$</li>
<li>This corresponds to the most probable class label, and is called themodeof the distribution p(y|x,D); it is also known as a <em>MAP</em> estimate(MAP stands for maximum a posteriori).</li>
<li>note that p(y|x,D) is the probrbility distribution of y given x and D (and M in case of in the situation of model selection, M should also be clearified)</li>
</ul>
<p>#Murphy’s definition of probabilistic model in machine learning#</p>
<ul>
<li>when defining a models, if the number of parameter is:<ol>
<li>grow with the amount of training data :=. non-parametric model<ul>
<li>flexible but computation cost high</li>
<li>eg: KNN<ul>
<li>KNN probabilistic model: </li>
<li>$p(y=c|\mathbf{x},D,K) = 1/K \Sigma_{i in N_K(x,D)} II(y_i=c)$</li>
<li>NK(x,D)are the (indices of the)Knearest points toxin DandI(e)is the indicator function </li>
<li>issure: fail to work in high D use to below:<ul>
<li>the curse of dimensionality (murphy ch1)</li>
</ul>
</li>
<li>explanation:<ul>
<li>lets say, apply KNN to uniformly distributed points in 3 dimensional unit cube grow a hyper-cube around x untill it contain 0.5 of the data points, $V(hyper-cube)=e_3(0.5)^3=0.5$, $e_3(0.5) = 0.5^{1/3}$, </li>
<li>for D dimension, given disired faction f, length of cube around x is $e_D(f)=f^{1/D}$</li>
</ul>
<ul>
<li>$e_{10}(0.1)=0.8$: D=10, want 0.1 of the datas, length=0.8, ie, need to extend the cube 80% along each dimension around x, which means that, the ‘nearest nerighbor’ is indeed not that near since that on any dimendion, their distance can be 0.8 and the largest distance is only 1; </li>
</ul>
</li>
</ul>
<ul>
<li>combat by making assumption about the nature of data distribution – know as <em>inductive bias</em> embodied in parametric model</li>
</ul>
</li>
</ul>
</li>
<li>fixed :=&gt; paramteric <ul>
<li>ads: fast to apply</li>
<li>dis: strong assumption about the nature of data distribute</li>
<li>regression<ul>
<li>linear regression:<ul>
<li>model: $y(x)=w^Tx + \episilon$</li>
</ul>
<ul>
<li>assume $e~N(\mu,\sigma^2)$ connect linear regre and Gaussian:<ul>
<li>model: $p(y|\mathbf{x},\mathbf{w},\sigma^2)=N(t|\mathbf{w}^T \phi(\mathbf{x}),\sigma^2(x))$</li>
<li>$\phi(\mathbf{x})$ basis function expansion, usually it is a vector of different degree of x – in polynomial regression</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>classification:<ul>
<li>logistic regression, ber distribute, use sigmoid function as basis function. <ul>
<li>define decision rule + basi function expansion -&gt;create decision boundary </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<p>#overfitting and model selection#</p>
<ul>
<li>overfit is an issue raised in both unsurpervised learning and surpervised learning during the selection of model</li>
<li>examples is:<ul>
<li>in linear regression, if choose polynormial as basis function, then the degress of the polynormial function distinct the model, the higher the degree is, it is more likely to overfit, which will lead to high error on the test set, since we use this error to estimate the generalization error, it means that the generalization increase<ul>
<li>as the degree raise, the complexity of the model increase, the matrx will tend to be <code>ill condition</code>, and the solution of parameters will become more sensitive to the training data, which is really bad! </li>
<li>also notice the fact that, as degree increase, the value of parameter also increase dramatically, in the example of PRML, D=1,w0=0.19 while for D=9,w9=125201 !</li>
<li><code>commend</code> someone said when we are not sure about the model, we should always choose the simplier model to fit the data</li>
<li><code>commend</code> we can still have some way to use complex model and reduce the generalization error</li>
<li><code>overfitting problam can be regards as a general property of maximum likelihood</code>, the matrix tend to be always in ill-condition since that the data points can not be completely in linear relation in reality.</li>
<li>PRML said <code>by adopting Bayesian approach, the overfitting problem can be avoided and the num of parameters can even greatlt exceed the num of data point</code></li>
</ul>
<ul>
<li>solution1: <code>shrinkage method</code> / <code>regularization</code> to fight with overfitting, usually we will try to penilize the parameters of the model, which perform <code>lift of the eigrnvalue</code>, in this way, we are actually trying to simplify the models, (thinking that when the panelized parameter $\lambda$ bacomre very large, the parameters, except $\w_0$ which is nor panelized, are all going to zero, so we are using a very simple model in fact). <ul>
<li>if add regularization, the complexity of the model should not be measured simply by the number of parameters or the degrees</li>
<li>the method above is called <em>ridge regression</em></li>
<li>we should also notice that the panelized parameter $\lambda$ could very a lot with the change the degree, from my experiment, it can very in the 1 ~ 10^7 range as the degree goes from about 2 to 10. So when perform cross validation on the selection of the model, more specifically, on the decision of penalized perameter and the degree, we need to pay attention about this</li>
</ul>
</li>
</ul>
</li>
<li>in KNN, it also happens that when K is small, it tends to overfit.</li>
</ul>
</li>
</ul>
<hr>
<p><em>speed-accuracy-complexity tradeoff</em><br>in the following notes, I will focus on the comparison of the tradeoff</p>
<ul>
<li>indicator function: $II(e)=1,if e is true; =2 if e is false$</li>
</ul>
<hr>
<p>#multinomial distribution#</p>
<ul>
<li>MODEL: tossing K side die </li>
<li>$\mathbf{x}=(x_1,…,x_D)$, $x_j$ the number of times side x of the die occurs</li>
<li>notice that $N = sum(x_1,x_2,…x_D)$</li>
<li>ANOTHER MODEL: from D color, choose fill in N place(not consider the order, ie,red in place1+blue in place2=red in 2+blue in 1), $x_j$ the number of color j shows in the image. probability that j is chosen = $\theta_j$</li>
<li>$Mu(\mathbf{x}|N,\mathbf(\theta)) := \binom{N}{x_1} \binom{N-x_1}{x_2}..\binom{N-x_1-x_2-…}{x_D} * \theta_1^{x_1} \theta_2^{x_2}…\theta_D^{x_D}$</li>
<li>$= \binom{N}{x_1 x_2 ..x_D} \prod{\substack{j=1,2,…D}} \theta_j^{x_j}$</li>
</ul>
<p>#multi-noulli#</p>
<ul>
<li>simply let N = 1;</li>
<li>$Mu(\mathbf{x}|1,\mathbf(\theta)) := \prod{\substack{j=1,2,…D}} \theta_j^{II(x_j=1)}$ II for 0,1 case</li>
</ul>
<hr>
<p>#information theory#</p>
<ul>
<li>deta compression/ source coding, concerned with: compact data representation and errorless transmission and storing </li>
</ul>
<h2 id="entropy">entropy</h2><ul>
<li>for a random variable Y(with K state) with ditribution p, the entropy of Y is denoted by </li>
<li>$\mathbb{H}(Y)$ or $\mathbb{H}(p)$ </li>
<li>$:= - \sum_{\substack{k=1,2,….K}} p(Y=k) log_2 p(Y=k)$</li>
<li>is a measure of its uncertainty<ul>
<li>say if $X \in {1,…,5}$ then if p = [1/5,1/5,1/5,1/5,1/5] the abs(entropy) reach maximum and in this case, entropy(&lt;0) is minimum, therefore uncertainty is largest when p = [1,0,0,0,0] the netropy reach minimum - no uncertainty</li>
</ul>
</li>
</ul>
<hr>
<p>regular expression help to replace the wrong mathjex:<br>eg find: (mathbf)((.))<br>replace with \1{\2}</p>
</span>
      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/10/18/useful-git-command/" rel="next">useful git command</a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/xx.jpg" alt="XXXH" itemprop="image"/>
          <p class="site-author-name" itemprop="name">XXXH</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">6</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">0</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">3</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#entropy"><span class="nav-number">1.</span> <span class="nav-text">entropy</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XXXH</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.1" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
