<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>




  <meta name="keywords" content="gradient descent,least square,machine learning,solver," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="Linear regression is the basis of machine learning.Basically, to compute the linear regression model, usually we will first look at the cost and try to minimize it. Gradient descent/ Follow the gradie">
<meta property="og:type" content="article">
<meta property="og:title" content="solving linear regression">
<meta property="og:url" content="http://yoursite.com/2015/10/18/solving-linear-regression/index.html">
<meta property="og:site_name" content="XXXH">
<meta property="og:description" content="Linear regression is the basis of machine learning.Basically, to compute the linear regression model, usually we will first look at the cost and try to minimize it. Gradient descent/ Follow the gradie">
<meta property="og:image" content="http://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec5_steep_desce.pdf">
<meta property="og:updated_time" content="2015-11-03T15:55:08.943Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="solving linear regression">
<meta name="twitter:description" content="Linear regression is the basis of machine learning.Basically, to compute the linear regression model, usually we will first look at the cost and try to minimize it. Gradient descent/ Follow the gradie">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>



  <title> solving linear regression | XXXH </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">XXXH</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            Tags
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              solving linear regression
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-18T22:04:32+02:00" content="2015-10-18">
            2015-10-18
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><p>Linear regression is the basis of machine learning.<br>Basically, to compute the linear regression model, usually we will first look at the cost and try to minimize it. Gradient descent/ Follow the gradient is a general appraoch. And there are two slovers for the optimization problem: least square &amp; normal equation.<br><a id="more"></a></p>
<table>
<thead>
<tr>
<th>least square using Gradient descent</th>
<th>least square using normal equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>convergent order = 1, linear convergent</td>
<td></td>
</tr>
<tr>
<td>minimize function of $\alpha$ iteratively, search algorithm</td>
<td>directly</td>
</tr>
<tr>
<td>cal gradient, step by step: update rule, which is call ‘least mean squares/LMS update rule’</td>
<td>set first derivative = 0, generate normal equation -&gt; closed form solution: $\theta = (X^TX)^{−1}X^T \mathbf{y}$</td>
</tr>
<tr>
<td>may or may not the whole set of data</td>
<td>need whoe set of data</td>
</tr>
<tr>
<td></td>
<td>ill condition matrix happens</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>batch</th>
<th>stochastic</th>
</tr>
</thead>
<tbody>
<tr>
<td>direction caled based on all of the data</td>
<td>caled take randomly take one pair of data</td>
</tr>
<tr>
<td>convex, promise to reach global minimum</td>
<td>close to global minimum but can not reach</td>
</tr>
<tr>
<td>high computation cost, can be modified by ‘rocking’</td>
</tr>
</tbody>
</table>
<h1 id="Gradient_descent">Gradient descent</h1><ul>
<li>a common solver in machine learning:</li>
<li>offline learning: having a batch of data, optimaze the equation(average loss, called rish in frequentist decision theory) base on all the data – batch gradient descent</li>
<li>online learning: having a stream of data, update the estimate with new coming data</li>
</ul>
<p>##BATCH GRADIENT DESCENT(offline learning)</p>
<ul>
<li>one parameter: (singularle variable function optimization) $\beta^{(k+1)} \gets \beta^{(k)} - \alpha  \frac{\partial \mathcal{L}(\beta)^{(k)}}{\partial \beta}$ （stepsize $\alpha$ is hard to determine mannully）</li>
<li>multi-parameter:(multivariable function optimization) $\mathbf{\beta}^{(k+1)} \gets \mathbf{\beta}^{(k)} - \alpha  \frac{\partial \mathcal{L}(\mathbf{\beta})^{(k)}}{\partial \mathbf{\beta}}$<br>= $\mathbf{\beta}^{(k)} - \alpha  \nabla \mathcal{L}(\mathbf{\beta})^{(k)} $</li>
<li>dis: require large memory<ul>
<li>a trick to speed up: <code>rocking</code> pass data forward-backward</li>
</ul>
</li>
<li>ads: could reach global minimum</li>
</ul>
<h2 id="stochastic_gradient_descent">stochastic gradient descent</h2><ul>
<li>choose random pair in hte traning set and take a step, </li>
</ul>
<hr>
<p>implement issue</p>
<h3 id="direction_selection">direction selection</h3><ul>
<li>beside choosing the step size, we can also choose the direction, originally we use the steepest-descent methos which is follow the gradient direction, but this way is usually have a low convergent order + numerically not convergent there are other direction can be use – more general descent methods</li>
<li>say direction $p^k$, gradient: $\nabla f(\mathbf{\beta})$, the requirement for the p is :$\nabla f(\mathbf{\beta}) ^T p^k &lt; 0 $(in the nearly opposite direction)</li>
</ul>
<h3 id="convergence_of_the_method_-_analysis_of_gradient_descend">convergence of the method - analysis of gradient descend</h3><ul>
<li><code>when is guaranteed for the method to converge?</code> ?: $\mathcal{L}(\beta^{(k)})$ is continuous differentiable in the bounded set {$\beta | \mathcal{L}(\beta) &lt; \mathcal{L}(\beta_0)$} – refer the [convergent theorem][1]<ul>
<li>convex function have only one global minimun(all local minima are also global minimum); strictly convex function have unique global minimum; </li>
<li>sequence {$\nabla \mathcal{L}(\beta^{(k)})$} </li>
</ul>
</li>
<li><code>convergent order</code> , <code>linear convergent</code> , <code>convergent constant</code> [see here][2] <ul>
<li><strong>meaning of convergent constant in gradient descend:</strong> if =0.1, then te itration gain 0.1 of accuracy in the <code>optimal objective function value</code> each round, if =0.9, need $0.9^{22}$, ie 22 round<ul>
<li>convergent constant small is better, even both in linear convergent, different c.c can means huge diff. performance</li>
<li>cal $cc = (\frac {\lambda<em>{max} - \lambda</em>{min}}{\lambda<em>{max} + \lambda</em>{min}})^2$ <img src="http://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec5_steep_desce.pdf" alt="see here"></li>
</ul>
</li>
<li>convergent rate can be increase by <code>feature scaling/data normalization/prepossing</code>, methods can be rescaling, standarization, or scling to unit length. <ul>
<li>standarization: x = (x - mean(x))/var(x)</li>
</ul>
</li>
</ul>
</li>
<li>if the problem is see here quadratic optimization problem , the model can be </li>
<li>QP: minimize $f(x) := 1/2 x^T Q x + c^T x$ which is a ellipse and Q is symmetry matrix, in this case, eigenvalue is the radius of the ellipse and <em>the convergent constant depends very much on the ratio of the largest to the smallest eigenvalue of the Hessian matrix H(x) at the optimal soluction $x^</em>$<em> <em>*we want the ratio to be small, that is , nearly 1</em></em> </li>
<li>ratio of largest to smallest eigenvalue = <strong>condition number</strong> of the matrix</li>
</ul>
<h3 id="Stopping_criteria_-_optimality_conditions">Stopping criteria - optimality conditions</h3><ol>
<li>first derivative = 0</li>
<li>second deri. positive definite. <code>what is this mean?</code></li>
</ol>
<h3 id="step-size_selection:_$\alpha$">step-size selection: $\alpha$</h3><ul>
<li>requirement:  <strong>Convergence</strong> to a local minimum is guaranteed only when $\alpha$ &lt; $\alpha<em>{min}$ where $\alpha</em>{min}$ is a fxed constant that depends on the problem. </li>
<li>$\alpha$ can be different each iteration(decreasing) or fixed<ul>
<li>we may hope $\alpha$ decrease as it approach the minimum, however, it can not simply decrease rach iteration otherwise it may not converge to the minimum, so we should besed on the value of each iteration to decide whether decrease or keep – automatically selection<br>method to find the locally optimal step size:<br>1.Exact Line Search: $\alpha^k$ is picked to minimize a function of $\alpha^{k}$ knowing x and d: $ = arg min_{\alpha}h(\alpha)= f(x^k + \alpha p^k)$ =&gt; usable, not cost effective</li>
</ul>
</li>
</ul>
<ol>
<li>Inexact line search:</li>
</ol>
<ul>
<li>bisection algorithm to soling the equation: $h’(\alpha)=0$<ul>
<li>s1: i=0, al=0,au=$\hat{a}$; s2: a = (al+au)/2; s3: if h’(a)&gt;0, au=a,k++, else, al=a,k++; stop whrn h’(a)=0</li>
</ul>
</li>
<li>line-search method: used to set step-size automatically: <code>backtracking</code><ul>
<li>[MIT nonlinear programming][3] </li>
<li>[line-search notes][4]  </li>
<li>backtracking (Armijo) line search:<ul>
<li>in a word, reduce step size after making sure that value of f is smaller than before</li>
<li>1) $\alpha^{(0)} = \alpha_{init}$ was given $l=0$, 2) if $f(x^k + a^{(l)}p^k) “&lt;” f^k$, i) set $\alpha^{(l+1)} = t\alpha^{(l)} $ t in (0,1) is fixed and ii) increment l by 1  3)set $\alpha^k = \alpha{(l)}$ – prevent step size getting too small before approach the solution but it does not prevent step to be too long – improve by tighten the requirement for “&lt;” see the notes for modification and termination</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="comment_on_gradient_descent:">comment on gradient descent:</h1><p>as along as the cost function is convex and the first derivative is continuous, the gradient descent method is guaranteed to reach the global minimum (ie, convergent). But the fact is that gradent descent have problem with vally shape optimizaton, in which case the ratio of the maximal and minimal eigenvalues of X is large, the number of iterations to reduce the optimality cap can be very large, <a href="http://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec5_steep_desce.pdf" target="_blank" rel="external">http://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec5_steep_desce.pdf</a>, here shows that when the ratio vary from 1 to 100, the number of iterations vary from 10 to 2310,(mention that the convergent constant from 0.0023 t0 0.99, for what it means ,refer above)<br><strong>conclusion</strong>: steepest descent convergence rate is very sensitive to the eigenvalue ratio<br>which is saying that the features are not normalized well<br>speed of convergence of gradient descent depends on the<br>perfrom: feature scaling </p>
<hr>
<ul>
<li>Lipschitz function:<ul>
<li>a function with bounded first derivative</li>
</ul>
</li>
</ul>
<h4 id="condition_number">condition number</h4><ul>
<li><em><code>condition number</code> measure how easy it is to invert a matrix</em><ul>
<li>condition number of $\mathbf{A} = \parallel\mathbf{A^T}\parallel\times \parallel \mathbf{A} \parallel$ [link: ==][1] $\parallel \mathbf{A^{-1}} \parallel \parallel \mathbf{A} \parallel = \frac{\sigma<em>{max} (\mathbf{A})}{\sigma</em>{min} (\mathbf{A})}$</li>
<li>matrix norms: a way of sizing up a matrix, a small change to a matrix can change the norm in a great scale</li>
<li>large condition number of $\mathbf{A}$<br>= ill-conditioned/ill-posed problem<br>= inaccurate result<br>= hard to invert </li>
<li>view condition number as error multiplier of solution of linear system $\mathbf{A}x = b$</li>
</ul>
</li>
<li>One way that systems of equations can be made better conditioned is to fix things so that all the rows have largest elements that are about the same size - (normalization)</li>
<li>for an invertible matrix:<br>$\Delta b$ is the error of the collected data<br>$\mathbf{x^<em>}$ is the solution of the system<br>$\mathbf{A}x = b + \Delta b$<br>$\mathbf{x} = \mathbf{A^{-1}} (\mathbf{b} + \Delta \mathbf{b}) = \mathbf{A^{-1}} \mathbf{b} + \mathbf{A^{-1}} \Delta \mathbf{b} = \mathbf{x^</em>} + \mathbf{A^{-1}} \Delta \mathbf{b}$<br>intuitively, if $A$ is small then $A^{-1}$ is large</li>
</ul>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/gradient-descent/" rel="tag">#gradient descent</a>
          
            <a href="/tags/least-square/" rel="tag">#least square</a>
          
            <a href="/tags/machine-learning/" rel="tag">#machine learning</a>
          
            <a href="/tags/solver/" rel="tag">#solver</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/10/19/classification/" rel="prev">classification</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/10/18/useful-git-command/" rel="next">useful git command</a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/xx.jpg" alt="XXXH" itemprop="image"/>
          <p class="site-author-name" itemprop="name">XXXH</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">28</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">7</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">37</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ZENGXH" target="_blank">github</a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/xiao-xiao-hui-49" target="_blank">zhihu</a>
              </span>
            
              <span class="links-of-author-item">
                <a href="4650xh@gmail.com" target="_blank">email</a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Gradient_descent"><span class="nav-number">1.</span> <span class="nav-text">Gradient descent</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#stochastic_gradient_descent"><span class="nav-number">1.1.</span> <span class="nav-text">stochastic gradient descent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#direction_selection"><span class="nav-number">1.1.1.</span> <span class="nav-text">direction selection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convergence_of_the_method_-_analysis_of_gradient_descend"><span class="nav-number">1.1.2.</span> <span class="nav-text">convergence of the method - analysis of gradient descend</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stopping_criteria_-_optimality_conditions"><span class="nav-number">1.1.3.</span> <span class="nav-text">Stopping criteria - optimality conditions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#step-size_selection:_$\alpha$"><span class="nav-number">1.1.4.</span> <span class="nav-text">step-size selection: $\alpha$</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#comment_on_gradient_descent:"><span class="nav-number">2.</span> <span class="nav-text">comment on gradient descent:</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#condition_number"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">condition number</span></a></li></ol></li></ol></li></ol></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XXXH</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.1" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
