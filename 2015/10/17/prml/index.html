<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>




  <meta name="keywords" content="ml MLE," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="life is so hard.

logistic regression is not applied for regression
least mean square is a rule(the update rule for gradient descent)
least square is actually means: using MSE distance function, it ca">
<meta property="og:type" content="article">
<meta property="og:title" content="prml">
<meta property="og:url" content="http://yoursite.com/2015/10/17/prml/index.html">
<meta property="og:site_name" content="XXXH">
<meta property="og:description" content="life is so hard.

logistic regression is not applied for regression
least mean square is a rule(the update rule for gradient descent)
least square is actually means: using MSE distance function, it ca">
<meta property="og:image" content="https://www.quora.com/Intuitively-speaking-What-is-the-difference-between-Bayesian-Estimation-and-Maximum-Likelihood-Estimation">
<meta property="og:image" content="http://cs229.stanford.edu/notes/cs229-notes1.pdf">
<meta property="og:image" content="http://cs229.stanford.edu/notes/cs229-notes1.pdf">
<meta property="og:updated_time" content="2015-10-21T21:26:46.431Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="prml">
<meta name="twitter:description" content="life is so hard.

logistic regression is not applied for regression
least mean square is a rule(the update rule for gradient descent)
least square is actually means: using MSE distance function, it ca">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>



  <title> prml | XXXH </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">XXXH</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            Tags
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              prml
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2015-10-17T21:32:03+02:00" content="2015-10-17">
            2015-10-17
          </time>
        </span>

        

        
          
        
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><p>life is so hard.</p>
<ol>
<li>logistic regression is not applied for regression</li>
<li>least mean square is a rule(the update rule for gradient descent)</li>
<li>least square is actually means: using MSE distance function, it can be solve by Gradient descent or normal equation</li>
<li>maximum likelihood exrimate of $\theta$ is solve by minimize the negative of log-likehood function of $\theta$</li>
<li>likehood may be view intuitively as the likelihood of what y will be given x but when talking MLE it is actually the likelihood of what parameters will be gien x (and y, or distrubution of y)</li>
<li>sigmoid function is a magic(fourier is also a magic): $\sigma(x)’ =\sigma(x)(1-\sigma(x))$</li>
<li>if you hold strong prior belief, you will have high bias and low variance, the world will not affect you…</li>
<li>MLE can be really silly.. due to overfitting. <img src="https://www.quora.com/Intuitively-speaking-What-is-the-difference-between-Bayesian-Estimation-and-Maximum-Likelihood-Estimation" alt="see here"></li>
<li>under a set of assumptions(Gaussian, independent) least squares regression could be derived as the maximum likelihood estimator </li>
<li><h2 id="preprocessed">preprocessed</h2>for input may be <em>preprocessed</em><ul>
<li>like fixed size of the image</li>
<li>also called feature extraction</li>
<li>speed up computation</li>
<li>easier to solve problem</li>
</ul>
</li>
</ol>
<h1 id="reference_book">reference book</h1><ol>
<li>PRML:</li>
<li>machine learning, a probalistic approach: murphy</li>
<li>the elements of statistical learning:HTF</li>
<li><img src="http://cs229.stanford.edu/notes/cs229-notes1.pdf" alt="Andrew&#39;s notes"></li>
</ol>
<h1 id="OVERVIEW">OVERVIEW</h1><ul>
<li>for the <code>data input type</code> we can define the learning process as supervised or unsupervised</li>
<li>for the <code>type of prediction</code>, we can define the case as classification(discrete output) or regression(continuous output)</li>
<li>for the <code>fitting model</code> we choose, we can have linear model(basis function expansion), or rigistic regression(not regression but for classification actually)</li>
<li><p>analyze the model and find parameter – cost function:</p>
<ul>
<li><p>derectly define <code>cost function</code> based on distance such as MAE, MSE…</p>
<ul>
<li>whether the cost function is <code>robust</code>, ie, how they react to the <code>outlier</code> will also affect our model – statistic property</li>
<li>whether the cost function is <code>convex</code> relate whether we can reach the minimum if we count on the gradient to solve the minimum – computational property</li>
</ul>
<ul>
<li>just based on the cost function above is not enough, we can also take another approach..</li>
</ul>
</li>
<li><p>define from <code>probabilistic view</code> of machine learning, we try to seek support for our modeling precess, analyze the model and tuning the model,  </p>
<ul>
<li><p><code>probability theory</code> can be applied during the prediction, model selection, measurement to be performed</p>
</li>
<li><p>for regression, we focus on the distribution of the output data or error </p>
<ul>
<li>by assume the error following the Gaussian distribution, the value we try to estimate/predict also follow the Gaussian distribution which take the y_prediction as mean.</li>
</ul>
</li>
<li>while for classification, we take the Bayisan theorm – and then generate the likehood function</li>
<li>we can use the joint probability assuming the data is indenpendent</li>
</ul>
</li>
<li><p>WHY cost function so important:</p>
<ul>
<li>since that the parameter is choosen by the score return by cost function </li>
</ul>
</li>
</ul>
</li>
<li>consider whether the <code>parameter is fixed or changed</code> with the size of data, the model can be <code>parametric</code>(linear regression(oridinary), logistic…) or <code>nonparametric</code>(<img src="http://cs229.stanford.edu/notes/cs229-notes1.pdf" alt="Locally weighted linear regression">(only meet at andrew’s, not the linear-regress we saw),KNN)       </li>
<li>for <code>optimation</code> numerically ie. <code>solve the minimum</code> of function in which we regard the parameter instead of the data as variable, we have: gradient descent, grid search, least square(closed form soluction, Gram matrix), newton method. for the solver, we need to consider the <code>compuration complexity</code> and <code>convexity</code><ul>
<li>implementation issue: – see reference: non-linear programming</li>
<li>gradient descent: feature normalization, direction(batch gradient descent, stochastic), stepsize(fixed, line-search:backtracking), convexity and converge, stopping criteria(optimal condition, positive definite)</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>supervised learning<ul>
<li>classification- discrete output</li>
<li>regression- continues output</li>
<li><code>generalization</code> – analyzing model<ul>
<li>:= make predictions on novel input</li>
<li>generalization error = expected value of the misclassification rate when averaged over future data, can never know but can be approximate by:  misclassification rate on a large independenttest set</li>
</ul>
</li>
</ul>
</li>
<li>unsupervised learning<ul>
<li>goal: knowledge discovery or discover interesting sreuctre in data in murphy</li>
</ul>
<ul>
<li>visualization</li>
<li>clustering</li>
<li>density estimation<br>…<br>-reinforcement learning</li>
<li>find suirable actions to take in a given stituation in order to maximized the reward</li>
</ul>
</li>
</ul>
<h2 id="linear_model">linear model</h2><ul>
<li>function which is linear in unknow parameters are called <code>linear model</code><ul>
<li>for the <code>polynomial model</code>: </li>
<li>$y(x, \mathbf{\beta}) = \beta^0 + \beta_1 x + \beta_2 x^2 + …. + \beta_M x^M$ </li>
<li>notices that the basis function(polynomial function) is <em>nonliear</em> in input x but linear in unknow parameter $\beta$</li>
<li>implement issue/ model selection:[detail in <em>ovrifitting</em> part]<ul>
<li>choose the value of coefficents/parameters/weights is determined by fitting data - minimizing error function - <em>cost function</em></li>
<li>choose the order/degree M - model selection/comparision<ul>
<li>overfitting </li>
<li>regulazation</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>in linear regression: take $\phi$ as veriable – linear; but the $\phi$ function itself can be nonliear</li>
</ul>
<hr>
<h1 id="probability_theory">probability theory</h1><ul>
<li>提出probability 是想更加科学一些</li>
<li><em>expectation</em> <ul>
<li>def: weighted averages of funtions</li>
<li>if we are given a finite number N of points drawn from the probability distribution or probability density<ul>
<li>$E[f] ~- 1/N sum_{n=1,2…N}f(x_n)$       </li>
</ul>
</li>
<li>consider expectation of functions of several variables eg f(x,y)<ul>
<li>expectation have subscript with repesct to different variable:</li>
<li>$E_x[f(x,y)]$ and $E_y[f(x,y)]$</li>
</ul>
</li>
</ul>
</li>
<li>variance: $Var[x] = E[x^2] - E[x]^2$<ul>
<li>consider functions of several variables eg f(x,y)<ul>
<li><em>covariance</em>: $cov[x,y] = E_{x,y}[{x - E[x]}{y^T - E[y^T]}]$  </li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="background:_interpretation_of_probabilities">background: interpretation of probabilities</h2><h3 id="frequentist_interpretation">frequentist interpretation</h3><ul>
<li>popular: <em>classical or frequentist way</em><ul>
<li>the probability P of an uncertain event A, P(A) is defined by the frequency of that event based on previous observations</li>
<li>In the frequentist this view of the world, θ is not random—it just happens to be unknown—and it’s our job to come up with statistical procedures (such as maximum likelihood) to try to estimate this parameter.(NG)</li>
</ul>
</li>
</ul>
<h3 id="Bayesian_interpretation">Bayesian interpretation</h3><ul>
<li>another point of view: <em>Bayesian view</em> - <code>probability provide a quanrification of uncertainty</code><ul>
<li>ads: can be used to model our uncertainty of the event without long term frequency</li>
<li>for future event, we do not have historical database thus can not count the frequency.</li>
<li>can measure the belief in a statement $a$ based on some ‘knowledge’, denote as P(a|K), different K can generate different P(a|K) and even same K can have different P(a|K) – the belief is subjective </li>
</ul>
</li>
</ul>
<h4 id="Bayes_rule">Bayes rule</h4><ul>
<li>consider <code>conditional probabilities</code> </li>
<li>$P(A|B) = \frac {P(B|A) P(A)}{P(B)}$<ul>
<li>interpretation: updating our belief about a hypothesis A in the light of new evidence B<ul>
<li>in <code>likelihood</code>, it is, output brief of y/A given B/input values+paramters </li>
</ul>
</li>
<li>P(A|B): posterior belief</li>
<li>P(A): prior belief</li>
<li>P(B|A): likelihood, ie the B(our model) will occur if A(the output value of the sample data) is true.</li>
<li>P(B) is computed by: $\sum_{i=1,2,…}P(B|A_i)P(A_i)$ by <em>marginalisation</em>.</li>
<li>if you still remember the ‘cancer-test examples’ in statistic course, then p(A) is p(cancer), p(B|A) is p(positive|cancer), p(A|B) is p(cancer|positive) which is the value patient care for(but in fact we dont know) </li>
</ul>
</li>
</ul>
<h2 id="different_view_of_likelihood_function">different view of likelihood function</h2><ul>
<li>likelihood function: $P(\mathbf{y}|\mathbf{\beta})$</li>
</ul>
<h3 id="from_frequentist_way_of_interpretation:">from frequentist way of interpretation:</h3><ul>
<li>parameter $\beta$ is a fixed parameter, the value is determined by ‘estimator’</li>
<li>A widely used <code>frequentist estimator</code> is <em>maximum likelihood</em>, in which we set the value that maximizes the likelihood function </li>
<li>ie. choosing $\beta$ s.t. probability of the observed data is maximized(seems to be the MAP estimate introduced in Murphy’s book, see below)</li>
<li>in practice, use negative log of likelihood function = log-likelihood:= error function(monotonically decreasing)</li>
<li><strong>One approach to determining frequentist error bars is the bootstrap</strong>, <ul>
<li>s1: 就是在已有的dataset(size N)里面random弄出L个dataset(size N) by drawing data from 已有的dataset(抽取方式是，可以重复抽，可以有的没有被抽中)</li>
<li>s2: looking at the variability of predictions between the different bootstrap data sets. then evaluate the accuracy of the estimates of the parameter</li>
</ul>
</li>
<li>drawback: may lead to extreme conclusion if the dataset is bad, eg, a fair-looking coin is tossed three times and lands heads each time. in this case, we will generate parameter $\beta$ to make P(lands head) = 1   </li>
</ul>
<h3 id="from_Bayesian_viewpoint:">from Bayesian viewpoint:</h3><p>in machine learning, Bayes theorem is used to convert a priot ptobability $P(A) = P(\mathbf{\beta})$ into a posterior probability $P(A|B) = P(\mathbf{\beta}|y)$ by incorpoating the evidence provided by the observed data </p>
<ul>
<li>for $\mathbf{\beta}$ in the <code>polynormial curve fitting model</code>, we can take an approach with <code>Bayes theorem</code>:</li>
<li>$P(\mathbf{\beta} | \mathbf{y}) =\frac{ P(\mathbf{y}|\mathbf{\beta}) p(\mathbf{\beta})}{P(\mathbf{y})} $<ul>
<li>given data {y_1,y_2,…}, we want to know the $\beta$, cant get directly. $P(\mathbf{\beta} | \mathbf{y})$:= <strong>posterior probability</strong></li>
<li>$P(\beta)$:= prior probability; our assumption of $\beta$</li>
<li>${P(\mathbf{y})}$:= normalization constant since the given data is fixed</li>
<li>$P(\mathbf{y}|\mathbf{\beta})$:= <strong>likelihood function</strong>;<ul>
<li>can be view as function of parameter $\beta$ </li>
<li>not a probability distrubution, so intergral w.r.t $\beta$ not nessary = 1</li>
</ul>
</li>
<li>state Bayes theorem as : <strong>posterior 8 likelihood × prior</strong>, consider all of these as function of parameters $\beta$<ul>
<li>about the choice of the parameters <code>murphy have detail description in chapter3</code> but should notice that murphy have different notation for the relation above…<ul>
<li>intergrate both side base on $\beta$: $p(y)= \integral p(y|\beta)p(\beta)d\beta$</li>
<li>issue: particularly the need to marginalize (sum or integrate) over the whole of parameter space</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Murphy’s_explanation_–_MAP(maximum_a_posterior)_estimate">Murphy’s explanation – MAP(maximum a posterior) estimate</h3><ul>
<li>look at probability is conditional on the test input x, as well as the training set D, and the form of model M that we use to make predictions</li>
<li>Given a probabilistic output, we can always compute our “best guess” as to the “true label” using: <ul>
<li>$\hat y = \hat f(\mathbf{x} = argmax_{c=1..C} p(y = c|\mathbf{x},D,M)$</li>
<li>This corresponds to the most probable class label, and is called the mode of the distribution p(y|x,D); it is also known as a <strong>MAP estimate</strong>(maximum a posterior). </li>
</ul>
</li>
<li>note that p(y|x,D) is the <code>probrbility distribution of y given x and D</code> (and in case of in the situation of model selection, M should also be clearified, write as p(y|x,D,M))</li>
</ul>
<hr>
<h2 id="Murphy’s_definition_of_probabilistic_model_in_machine_learning">Murphy’s definition of probabilistic model in machine learning</h2><p>when defining a models, if the number of parameter is:</p>
<ol>
<li><p><code>grow with the amount of training data =&gt; non-parametric model</code></p>
<ul>
<li>flexible but computation cost high</li>
<li>eg: KNN<ul>
<li>KNN probabilistic model: </li>
<li>$p(y=c|\mathbf{x},D,K) = 1/K \Sigma_{i in N_K(x,D)} II(y_i=c)$</li>
<li>NK(x,D)are the (indices of the)Knearest points toxin DandI(e)is the indicator function </li>
<li>issure: fail to work in high D due to <code>curse of dimensionality</code>[SEE BELOW]</li>
</ul>
</li>
</ul>
</li>
<li><p><code>fixed =&gt; paramteric</code> </p>
<ul>
<li>ads: fast to apply</li>
<li>dis: strong assumption about the nature of data distribute</li>
<li>regression<ul>
<li>linear regression:<ul>
<li>model: $y(x)=w^Tx + \episilon$</li>
</ul>
<ul>
<li>assume $e~N(\mu,\sigma^2)$ connect linear regre and Gaussian:<ul>
<li>model: $p(y|\mathbf{x},\mathbf{w},\sigma^2)=N(t|\mathbf{w}^T \phi(\mathbf{x}),\sigma^2(x))$</li>
<li>$\phi(\mathbf{x})$ basis function expansion, usually it is a vector of different degree of x – in polynomial regression</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>classification:<ul>
<li>logistic regression, ber-distribute, use sigmoid function as basis function. <ul>
<li>define decision rule + basi function expansion -&gt;create decision boundary </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="the_curse_of_dimensionality_(murphy_ch1)">the curse of dimensionality (murphy ch1)</h3><ul>
<li>KNN fail to work in high D</li>
</ul>
<ul>
<li>explanation:<ul>
<li>lets say, apply KNN to uniformly distributed points in 3 dimensional unit cube grow a hyper-cube around x untill it contain 0.5 of the data points, $V(hyper-cube)=e_3(0.5)^3=0.5$, $e_3(0.5) = 0.5^{1/3}$, </li>
<li>for D dimension, given disired faction f, length of cube around x is $e_D(f)=f^{1/D}$</li>
</ul>
<ul>
<li>$e_{10}(0.1)=0.8$: D=10, want 0.1 of the datas, length=0.8, ie, need to extend the cube 80% along each dimension around x, which means that, the ‘nearest nerighbor’ is indeed not that near since that on any dimendion, their distance can be 0.8 and the largest distance is only 1; </li>
</ul>
</li>
</ul>
<ul>
<li>combat by making assumption about the nature of data distribution – know as <em>inductive bias</em> embodied in parametric model</li>
</ul>
<hr>
<h2 id="overfitting_and_model_selection">overfitting and model selection</h2><ul>
<li>overfit is an issue raised in both unsurpervised learning and surpervised learning during the selection of model</li>
<li>PRML said <code>by adopting Bayesian approach, the overfitting problem can be avoided and the num of parameters can even greatlt exceed the num of data point</code></li>
<li><code>overfitting problam can be regards as a general property of maximum likelihood</code>, the matrix tend to be always in ill-condition since that the data points can not be completely in linear relation in reality.</li>
</ul>
<h3 id="overfitting_in_linear_regression">overfitting in linear regression</h3><ul>
<li>in linear regression, if choose polynormial as basis function, then the degress of the polynormial function distinct the model, the higher the degree is, it is more likely to overfit, which will lead to high error on the test set, since we use this error to estimate the generalization error, it means that the generalization increase<ul>
<li>as the degree raise, the complexity of the model increase, the matrx will tend to be <code>ill condition</code>, and the solution of parameters will become more sensitive to the training data, which is really bad! </li>
<li>also notice the fact that, as degree increase, the value of parameter also increase dramatically, in the example of PRML, D=1,w0=0.19 while for D=9,w9=125201 !</li>
<li><code>commend</code> someone said when we are not sure about the model, we should always choose the simplier model to fit the data</li>
<li><code>commend</code> we can still have some way to use complex model and reduce the generalization error</li>
</ul>
</li>
</ul>
<h4 id="solution1:_shrinkage_method_/_regularization_to_fight_with_overfitting">solution1: <code>shrinkage method</code> / <code>regularization</code> to fight with overfitting</h4><ul>
<li><p>usually we will try to penilize the parameters of the model, which perform <code>lift of the eigrnvalue</code>, in this way, we are actually trying to simplify the models, (thinking that when the panelized parameter $\lambda$ bacomre very large, the parameters, except $\w_0$ which is nor panelized, are all going to zero, so we are using a very simple model in fact). </p>
<ul>
<li>involves adding a penalty term to the error function (1.2) in order to discourage the coefficients from reaching large values</li>
</ul>
<ul>
<li>add $\lambda / 2 | \mathbf{\beta} |^2$ $\lambda$ is the importance of the regulazation term; <ul>
<li>if use quadric regularizer - call <em>ridge regression</em></li>
<li>other regulazation such as <em>weight decay</em> in neural network </li>
</ul>
</li>
<li>$\lambda$ can suppressed over-fitting, reduce value of weight, but if $\lambda$ too large, weight goes to 0, will lead to poor fit</li>
</ul>
<ul>
<li>we should also notice that the panelized parameter $\lambda$ could very a lot with the change the degree, from my experiment, it can very in the 1 ~ 10^7 range as the degree goes from about 2 to 10. So when perform cross validation on the selection of the model, more specifically, on the decision of penalized perameter and the degree, we need to pay attention about this</li>
<li>after adding regularization, the complexity of the model should not be measured simply by the number of parameters or the degrees</li>
</ul>
</li>
</ul>
<h3 id="overfitting_in_KNN,">overfitting in KNN,</h3><ul>
<li>it also happens that when K is small, it tends to overfit.</li>
</ul>
<hr>
<h2 id="speed-accuracy-complexity_tradeoff">speed-accuracy-complexity tradeoff</h2><p>in the following notes, I will focus on the comparison of the tradeoff</p>
<ul>
<li>indicator function: $II(e)=1,if e is true; =2 if e is false$</li>
</ul>
<hr>
<h2 id="distribution">distribution</h2><h3 id="multinomial_distribution">multinomial distribution</h3><ul>
<li>MODEL: tossing K side die </li>
<li>$\mathbf{x}=(x_1,…,x_D)$, $x_j$ the number of times side x of the die occurs</li>
<li>notice that $N = sum(x_1,x_2,…x_D)$</li>
<li>ANOTHER MODEL: from D color, choose fill in N place(not consider the order, ie,red in place1+blue in place2=red in 2+blue in 1), $x_j$ the number of color j shows in the image. probability that j is chosen = $\theta_j$</li>
<li>$Mu(\mathbf{x}|N,\mathbf(\theta)) := \binom{N}{x_1} \binom{N-x_1}{x_2}..\binom{N-x_1-x_2-…}{x_D} * \theta_1^{x_1} \theta_2^{x_2}…\theta_D^{x_D}$</li>
<li>$= \binom{N}{x_1 x_2 ..x_D} \prod{\substack{j=1,2,…D}} \theta_j^{x_j}$</li>
</ul>
<h3 id="multi-noulli">multi-noulli</h3><ul>
<li>simply let N = 1;</li>
<li>$Mu(\mathbf{x}|1,\mathbf(\theta)) := \prod{\substack{j=1,2,…D}} \theta_j^{II(x_j=1)}$ II for 0,1 case</li>
</ul>
<hr>
<h2 id="information_theory">information theory</h2><ul>
<li>deta compression/ source coding, concerned with: compact data representation and errorless transmission and storing </li>
</ul>
<h3 id="entropy">entropy</h3><ul>
<li>for a random variable Y(with K state) with ditribution p, the entropy of Y is denoted by </li>
<li>$\mathbb{H}(Y)$ or $\mathbb{H}(p)$ </li>
<li>$:= - \sum_{\substack{k=1,2,….K}} p(Y=k) log_2 p(Y=k)$</li>
<li>is a measure of its uncertainty<ul>
<li>say if $X \in {1,…,5}$ then if p = [1/5,1/5,1/5,1/5,1/5] the abs(entropy) reach maximum and in this case, entropy(&lt;0) is minimum, therefore uncertainty is largest when p = [1,0,0,0,0] the netropy reach minimum - no uncertainty</li>
</ul>
</li>
</ul>
<hr>
<h2 id="feature_selection">feature selection</h2><p>motivation: having too many features and some of them are actuallt invariant, feature selection is belonging to model selection as well, introduced in notes 5 of Andrew NG</p>
<h2 id="regulazation">regulazation</h2><pre><code>- solve ill posed problem/ overfitting
<span class="number">1.</span> when talking <span class="keyword">about</span> MLE which <span class="keyword">is</span> alway facing overfitting problem, <span class="keyword">the</span> MAP <span class="keyword">and</span> baysian estimation <span class="keyword">is</span> improving <span class="keyword">it</span> <span class="keyword">with</span> prior belif which act <span class="keyword">as</span> regulazation 
</code></pre><hr>
<p>regular expression help to replace the wrong mathjex:<br>eg find: (mathbf)((.))<br>replace with \1{\2}</p>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ml-MLE/" rel="tag">#ml MLE</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/10/17/MLE/" rel="prev">MLE & MAP & Bayesian Estimation</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/10/17/handon-networking/" rel="next">handon-networking</a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/xx.jpg" alt="XXXH" itemprop="image"/>
          <p class="site-author-name" itemprop="name">XXXH</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">11</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">0</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">12</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#preprocessed"><span class="nav-number">1.</span> <span class="nav-text">preprocessed</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference_book"><span class="nav-number"></span> <span class="nav-text">reference book</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#OVERVIEW"><span class="nav-number"></span> <span class="nav-text">OVERVIEW</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#linear_model"><span class="nav-number">1.</span> <span class="nav-text">linear model</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#probability_theory"><span class="nav-number"></span> <span class="nav-text">probability theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#background:_interpretation_of_probabilities"><span class="nav-number">1.</span> <span class="nav-text">background: interpretation of probabilities</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#frequentist_interpretation"><span class="nav-number">1.1.</span> <span class="nav-text">frequentist interpretation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bayesian_interpretation"><span class="nav-number">1.2.</span> <span class="nav-text">Bayesian interpretation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bayes_rule"><span class="nav-number">1.2.1.</span> <span class="nav-text">Bayes rule</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#different_view_of_likelihood_function"><span class="nav-number">2.</span> <span class="nav-text">different view of likelihood function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#from_frequentist_way_of_interpretation:"><span class="nav-number">2.1.</span> <span class="nav-text">from frequentist way of interpretation:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from_Bayesian_viewpoint:"><span class="nav-number">2.2.</span> <span class="nav-text">from Bayesian viewpoint:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Murphy’s_explanation_–_MAP(maximum_a_posterior)_estimate"><span class="nav-number">2.3.</span> <span class="nav-text">Murphy’s explanation – MAP(maximum a posterior) estimate</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Murphy’s_definition_of_probabilistic_model_in_machine_learning"><span class="nav-number">3.</span> <span class="nav-text">Murphy’s definition of probabilistic model in machine learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the_curse_of_dimensionality_(murphy_ch1)"><span class="nav-number">3.1.</span> <span class="nav-text">the curse of dimensionality (murphy ch1)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#overfitting_and_model_selection"><span class="nav-number">4.</span> <span class="nav-text">overfitting and model selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#overfitting_in_linear_regression"><span class="nav-number">4.1.</span> <span class="nav-text">overfitting in linear regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#solution1:_shrinkage_method_/_regularization_to_fight_with_overfitting"><span class="nav-number">4.1.1.</span> <span class="nav-text">solution1: shrinkage method / regularization to fight with overfitting</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#overfitting_in_KNN,"><span class="nav-number">4.2.</span> <span class="nav-text">overfitting in KNN,</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#speed-accuracy-complexity_tradeoff"><span class="nav-number">5.</span> <span class="nav-text">speed-accuracy-complexity tradeoff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#distribution"><span class="nav-number">6.</span> <span class="nav-text">distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#multinomial_distribution"><span class="nav-number">6.1.</span> <span class="nav-text">multinomial distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-noulli"><span class="nav-number">6.2.</span> <span class="nav-text">multi-noulli</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#information_theory"><span class="nav-number">7.</span> <span class="nav-text">information theory</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#entropy"><span class="nav-number">7.1.</span> <span class="nav-text">entropy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#feature_selection"><span class="nav-number">8.</span> <span class="nav-text">feature selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regulazation"><span class="nav-number">9.</span> <span class="nav-text">regulazation</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XXXH</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.1" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
